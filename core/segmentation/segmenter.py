"""
Chunking destekli segmentasyon - Uzun metinler iÃ§in
"""
import os
import sys
import json
import time
import unicodedata
import re
from pathlib import Path
from datetime import datetime
import google.generativeai as genai
# Eski import'larÄ± llm/tools'tan al (backward compatibility)
import sys
from pathlib import Path
project_root = Path(__file__).resolve().parents[3]
sys.path.insert(0, str(project_root))
from llm.tools.gemini_segment import load_prompt, _extract_text, _repair_json, MODEL_NAME

# Chunking parametreleri
MAX_CHUNK_SIZE = 15000  # karakter
CHUNK_OVERLAP = 800  # Chunk'lar arasÄ± overlap (baÅŸlÄ±k kaybÄ±nÄ± Ã¶nlemek iÃ§in) - artÄ±rÄ±ldÄ±
MIN_FILL_RATIO = 0.65  # Minimum chunk doluluk oranÄ± (%65)


def split_text_into_chunks(text: str, chunk_size: int = MAX_CHUNK_SIZE, overlap: int = CHUNK_OVERLAP, min_fill_ratio: float = MIN_FILL_RATIO) -> list:
    """Metni chunk'lara bÃ¶l (baÅŸlÄ±k kaybÄ±nÄ± Ã¶nlemek iÃ§in akÄ±llÄ± bÃ¶lme)
    
    Ä°yileÅŸtirmeler:
    - Minimum doluluk eÅŸiÄŸi (%65)
    - Geriye dÃ¼ÅŸmeyi engelleme
    - Ã‡ift satÄ±r sonu (\n\n) â†’ tek satÄ±r sonu (\n) sÄ±rasÄ±yla yumuÅŸak kes
    """
    chunks = []
    text_len = len(text)
    min_chunk_size = int(chunk_size * min_fill_ratio)
    
    if text_len <= chunk_size:
        return [(0, text_len, text)]
    
    start = 0
    while start < text_len:
        end = min(start + chunk_size, text_len)
        original_end = end
        
        # EÄŸer son chunk deÄŸilse, satÄ±r sonu veya paragraf sonu bul
        if end < text_len:
            # Ã–nce Ã§ift satÄ±r sonu (\n\n) ara
            newline_pos = text.rfind('\n\n', start, end)
            if newline_pos > start + min_chunk_size:  # Minimum doluluk eÅŸiÄŸini koru
                end = newline_pos + 2
            else:
                # Tek satÄ±r sonu (\n) ara
                newline_pos = text.rfind('\n', start, end)
                if newline_pos > start + min_chunk_size:
                    end = newline_pos + 1
                # EÄŸer minimum doluluk eÅŸiÄŸi saÄŸlanamazsa, orijinal end'i kullan
        
        # Minimum doluluk kontrolÃ¼
        chunk_length = end - start
        if chunk_length < min_chunk_size and end < text_len:
            # Chunk Ã§ok kÃ¼Ã§Ã¼k, daha fazla al
            end = min(start + chunk_size, text_len)
        
        chunk_text = text[start:end]
        chunks.append((start, end, chunk_text))
        
        # Sonraki chunk iÃ§in baÅŸlangÄ±Ã§ (overlap ile) - GERÄ°YE DÃœÅMEYÄ° ENGELLE
        if end < text_len:
            # Overlap: sonraki chunk bir Ã¶nceki chunk'Ä±n sonundan overlap kadar geriye baÅŸlasÄ±n
            new_start = end - overlap
            
            # Ã–NEMLÄ°: Geriye dÃ¼ÅŸmeyi engelle - her zaman ileriye git
            prev_start = chunks[-1][0] if chunks else 0
            if new_start <= prev_start:
                # Geriye dÃ¼ÅŸtÃ¼yse, Ã¶nceki chunk'Ä±n sonundan 1 karakter ileri baÅŸla
                new_start = end
            else:
                # Overlap iÃ§inde yumuÅŸak bir kesme noktasÄ± bul
                # Ã–nce Ã§ift satÄ±r sonu, sonra tek satÄ±r sonu ara
                overlap_start = max(prev_start, new_start - overlap // 2)
                soft_break = text.rfind('\n\n', overlap_start, new_start)
                if soft_break > overlap_start:
                    new_start = soft_break + 2
                else:
                    soft_break = text.rfind('\n', overlap_start, new_start)
                    if soft_break > overlap_start:
                        new_start = soft_break + 1
            
            start = new_start
        else:
            break
    
    return chunks


def calculate_iou(start1: int, end1: int, start2: int, end2: int) -> float:
    """Ä°ki aralÄ±k arasÄ±ndaki Intersection over Union (IoU) hesapla"""
    intersection_start = max(start1, start2)
    intersection_end = min(end1, end2)
    
    if intersection_end <= intersection_start:
        return 0.0
    
    intersection = intersection_end - intersection_start
    union = (end1 - start1) + (end2 - start2) - intersection
    
    if union == 0:
        return 1.0 if start1 == start2 and end1 == end2 else 0.0
    
    return intersection / union


def normalize_section_name(name: str) -> str:
    """Section name'i normalize et (karÅŸÄ±laÅŸtÄ±rma iÃ§in)"""
    if not name:
        return ""
    # KÃ¼Ã§Ã¼k harfe Ã§evir, fazla boÅŸluklarÄ± temizle
    normalized = " ".join(name.lower().split())
    return normalized


def smart_dedup_sections(all_sections: list, iou_threshold_with_title: float = 0.6, iou_threshold_no_title: float = 0.8) -> list:
    """AkÄ±llÄ± duplicate temizleme - IoU ve baÅŸlÄ±k eÅŸleÅŸmesi ile
    
    Args:
        all_sections: TÃ¼m section'larÄ±n listesi
        iou_threshold_with_title: BaÅŸlÄ±k eÅŸleÅŸiyorsa IoU eÅŸiÄŸi (0.6)
        iou_threshold_no_title: BaÅŸlÄ±k yoksa IoU eÅŸiÄŸi (0.8)
    
    Returns:
        TemizlenmiÅŸ section listesi
    """
    if not all_sections:
        return []
    
    # Ã–nce start_idx'e gÃ¶re sÄ±rala
    sorted_sections = sorted(all_sections, key=lambda x: (x.get('start_idx', 0), x.get('end_idx', 0)))
    
    unique_sections = []
    seen_ids = set()
    id_counter = {}
    
    for section in sorted_sections:
        start = section.get('start_idx', 0)
        end = section.get('end_idx', 0)
        section_name = section.get('section_name', '')
        section_id = section.get('section_id', '')
        
        # Benzersiz ID kontrolÃ¼
        if section_id in seen_ids:
            # Yeni benzersiz ID oluÅŸtur
            base_id = section_id.rsplit('_', 1)[0] if '_' in section_id else section_id
            if base_id not in id_counter:
                id_counter[base_id] = 1
            else:
                id_counter[base_id] += 1
            section_id = f"{base_id}_{id_counter[base_id]}"
            section['section_id'] = section_id
        
        seen_ids.add(section_id)
        
        # AynÄ± section_id'yi normalize et
        normalized_name = normalize_section_name(section_name)
        
        # Mevcut section'larla karÅŸÄ±laÅŸtÄ±r
        merged = False
        for existing in unique_sections:
            existing_start = existing.get('start_idx', 0)
            existing_end = existing.get('end_idx', 0)
            existing_name = existing.get('section_name', '')
            existing_normalized = normalize_section_name(existing_name)
            
            # IoU hesapla
            iou = calculate_iou(start, end, existing_start, existing_end)
            
            # BaÅŸlÄ±k eÅŸleÅŸmesi kontrolÃ¼
            name_match = normalized_name and existing_normalized and normalized_name == existing_normalized
            
            # EÅŸik belirleme
            threshold = iou_threshold_with_title if name_match else iou_threshold_no_title
            
            # EÅŸik aÅŸÄ±ldÄ±ysa birleÅŸtir
            if iou >= threshold:
                # Daha uzun iÃ§eriÄŸe sahip olanÄ± tut
                existing_content_len = len(existing.get('content', ''))
                new_content_len = len(section.get('content', ''))
                
                if new_content_len > existing_content_len:
                    # Yeni section daha uzun, mevcut olanÄ± gÃ¼ncelle
                    unique_sections.remove(existing)
                    unique_sections.append(section)
                # Aksi halde mevcut olanÄ± tut (yeni section'Ä± atla)
                merged = True
                break
        
        if not merged:
            unique_sections.append(section)
    
    # Tekrar sÄ±rala
    unique_sections.sort(key=lambda x: x.get('start_idx', 0))
    
    return unique_sections


def validate_segmentation_result(sections: list, original_text: str) -> dict:
    """Mini validator - sessiz hatalarÄ± erken yakala
    
    Returns:
        {
            'valid': bool,
            'errors': list,
            'warnings': list
        }
    """
    errors = []
    warnings = []
    section_ids = set()
    
    for i, section in enumerate(sections):
        start = section.get('start_idx', 0)
        end = section.get('end_idx', 0)
        section_id = section.get('section_id', '')
        
        # start_idx < end_idx kontrolÃ¼
        if start >= end:
            errors.append(f"Section {i} ({section_id}): start_idx ({start}) >= end_idx ({end})")
        
        # SÄ±nÄ±rlar metin dÄ±ÅŸÄ±nda mÄ±?
        if start < 0:
            errors.append(f"Section {i} ({section_id}): start_idx ({start}) < 0")
        if end > len(original_text):
            errors.append(f"Section {i} ({section_id}): end_idx ({end}) > text length ({len(original_text)})")
        
        # Yinelenen section_id kontrolÃ¼
        if section_id in section_ids:
            errors.append(f"Duplicate section_id: {section_id}")
        section_ids.add(section_id)
    
    # SÄ±ralama sonrasÄ± bÃ¼yÃ¼k overlap kontrolÃ¼
    sorted_sections = sorted(sections, key=lambda x: x.get('start_idx', 0))
    for i in range(len(sorted_sections) - 1):
        curr = sorted_sections[i]
        next_sec = sorted_sections[i + 1]
        
        curr_end = curr.get('end_idx', 0)
        next_start = next_sec.get('start_idx', 0)
        
        if curr_end > next_start:
            overlap = curr_end - next_start
            curr_size = curr_end - curr.get('start_idx', 0)
            overlap_ratio = overlap / curr_size if curr_size > 0 else 0
            
            if overlap_ratio > 0.3:  # %30'dan fazla overlap
                warnings.append(
                    f"Large overlap between {curr.get('section_id')} and {next_sec.get('section_id')}: "
                    f"{overlap} chars ({overlap_ratio:.1%})"
                )
    
    return {
        'valid': len(errors) == 0,
        'errors': errors,
        'warnings': warnings
    }


def clean_model_output(output: str) -> str:
    """Model Ã§Ä±ktÄ±sÄ±nÄ± temizle - Unicode normalize ve kontrol karakterleri"""
    if not output:
        return output
    
    # Unicode normalize et (NFC)
    output = unicodedata.normalize("NFC", output)
    
    # Kontrol karakterlerini temizle - JSON'da geÃ§erli olanlar (\n, \r, \t) hariÃ§
    # Null karakterleri ve diÄŸer kontrol karakterleri sil
    cleaned = []
    for char in output:
        code = ord(char)
        if code == 0:
            # Null karakteri sil
            continue
        elif code < 32:
            # Sadece geÃ§erli kontrol karakterleri bÄ±rak (\n, \r, \t)
            if char in ['\n', '\r', '\t']:
                cleaned.append(char)
            # DiÄŸer kontrol karakterleri sil (JSON'da sorun yaratabilir)
            continue
        elif 127 <= code <= 159:
            # DEL ve diÄŸer kontrol karakterleri sil
            continue
        else:
            cleaned.append(char)
    
    return ''.join(cleaned)


def merge_segmentations(chunk_results: list, original_text: str) -> dict:
    """Chunk sonuÃ§larÄ±nÄ± birleÅŸtir"""
    all_sections = []
    
    for chunk_data in chunk_results:
        chunk_start_idx = chunk_data['chunk_start']
        sections = chunk_data['sections']
        
        # Section'larÄ±n start_idx ve end_idx'lerini orijinal metne gÃ¶re ayarla
        for section in sections:
            # Chunk iÃ§indeki relative pozisyonlarÄ± absolute pozisyonlara Ã§evir
            section['start_idx'] = chunk_start_idx + section.get('start_idx', 0)
            section['end_idx'] = chunk_start_idx + section.get('end_idx', 0)
            all_sections.append(section)
    
    # AkÄ±llÄ± duplicate temizleme (IoU ile)
    unique_sections = smart_dedup_sections(all_sections)
    
    # Parent_id'leri dÃ¼zelt (chunking sonrasÄ± parent_id'ler kaybolmuÅŸ olabilir)
    try:
        fix_seg_path = Path(__file__).parent / "fix_segmentation.py"
        if fix_seg_path.exists():
            sys.path.insert(0, str(Path(__file__).parent))
            from fix_segmentation import fix_missing_parents
            unique_sections = fix_missing_parents(unique_sections)
        else:
            raise ImportError("fix_segmentation.py not found")
    except (ImportError, Exception):
        # EÄŸer import edilemezse, basit bir dÃ¼zeltme yap
        all_ids = {s.get('section_id') for s in unique_sections}
        for i, sec in enumerate(unique_sections):
            level = sec.get('level', 1)
            parent_id = sec.get('parent_id')
            
            # GeÃ§ersiz parent_id'yi dÃ¼zelt
            if parent_id and parent_id not in all_ids:
                # En yakÄ±n geÃ§erli parent'Ä± bul
                for j in range(i - 1, -1, -1):
                    prev_sec = unique_sections[j]
                    if prev_sec.get('level') == level - 1:
                        sec['parent_id'] = prev_sec.get('section_id')
                        break
                else:
                    sec['parent_id'] = None
            
            # Level 2+ bÃ¶lÃ¼mler iÃ§in parent_id kontrolÃ¼
            if level > 1 and not sec.get('parent_id'):
                for j in range(i - 1, -1, -1):
                    prev_sec = unique_sections[j]
                    if prev_sec.get('level') == level - 1:
                        sec['parent_id'] = prev_sec.get('section_id')
                        break
    
    return {
        "segmentation": {
            "sections": unique_sections
        },
        "source_metadata": {
            "total_length": len(original_text),
            "extraction_timestamp": datetime.now().isoformat(),
            "chunked": True,
            "chunk_count": len(chunk_results)
        }
    }


def segment_text_chunked(text: str, api_key: str = None) -> str:
    """Uzun metinleri chunk'lara bÃ¶lerek iÅŸle"""
    api_key = api_key or os.getenv("GEMINI_API_KEY", "")
    if not api_key:
        raise RuntimeError("GEMINI_API_KEY is not set")
    
    # Chunk'lara bÃ¶l
    chunks = split_text_into_chunks(text)
    
    if len(chunks) == 1:
        # Tek chunk, normal segmentasyon
        from llm.tools.gemini_segment import segment_text
        return segment_text(text, api_key=api_key)
    
    print(f"ğŸ“¦ Metin {len(chunks)} chunk'a bÃ¶lÃ¼ndÃ¼ (her chunk ~{MAX_CHUNK_SIZE:,} karakter)")
    print()
    
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel(
        MODEL_NAME,
        generation_config={
            "temperature": 0,
            "response_mime_type": "application/json"
        },
    )
    
    chunk_results = []
    
    # Her chunk'Ä± iÅŸle
    for i, (chunk_start, chunk_end, chunk_text) in enumerate(chunks, 1):
        print(f"ğŸ”„ Chunk {i}/{len(chunks)} iÅŸleniyor... (pozisyon {chunk_start:,}-{chunk_end:,})")
        
        prompt = load_prompt().format(TEXT=chunk_text, SOURCE_LEN=len(chunk_text))
        
        # Chunk'Ä± iÅŸle
        max_retries = 3
        output = None
        
        for attempt in range(max_retries):
            try:
                resp = model.generate_content(prompt)
                output = _extract_text(resp).strip()
                break
            except Exception as e:
                if "429" in str(e) or "Resource exhausted" in str(e):
                    if attempt < max_retries - 1:
                        time.sleep(5 * (attempt + 1))
                        continue
                    else:
                        raise RuntimeError(f"Rate limit hatasÄ± (chunk {i})")
                else:
                    raise
        
        if not output:
            print(f"âš ï¸  Chunk {i} boÅŸ yanÄ±t dÃ¶ndÃ¼, atlanÄ±yor...")
            continue
        
        # Model Ã§Ä±ktÄ±sÄ±nÄ± temizle
        output = clean_model_output(output)
        
        # JSON parse et
        try:
            data = json.loads(output)
            sections = data.get('segmentation', {}).get('sections', [])
            chunk_results.append({
                'chunk_start': chunk_start,
                'sections': sections
            })
            print(f"   âœ… {len(sections)} bÃ¶lÃ¼m Ã§Ä±karÄ±ldÄ±")
        except json.JSONDecodeError as e:
            # Repair dene
            print(f"   âš ï¸  JSON parse hatasÄ±: {e}")
            print(f"   ğŸ”„ Repair deneniyor...")
            repaired = _repair_json(output)
            try:
                data = json.loads(repaired)
                sections = data.get('segmentation', {}).get('sections', [])
                chunk_results.append({
                    'chunk_start': chunk_start,
                    'sections': sections
                })
                print(f"   âœ… {len(sections)} bÃ¶lÃ¼m Ã§Ä±karÄ±ldÄ± (repair ile)")
            except json.JSONDecodeError as e2:
                print(f"   âŒ Chunk {i} repair ile de parse edilemedi: {e2}")
                print(f"   ğŸ”„ Retry ile tekrar deneniyor...")
                
                # Retry ile tekrar dene
                time.sleep(2)
                try:
                    retry_prompt = prompt + "\n\nCRITICAL: Output MUST be valid JSON. All strings must be properly escaped. No markdown, ONLY valid JSON."
                    resp2 = model.generate_content(retry_prompt)
                    output2 = _extract_text(resp2).strip()
                    
                    # Model Ã§Ä±ktÄ±sÄ±nÄ± temizle
                    output2 = clean_model_output(output2)
                    
                    # Markdown temizleme
                    if "```json" in output2:
                        start = output2.find("```json") + 7
                        end = output2.find("```", start)
                        if end != -1:
                            output2 = output2[start:end].strip()
                    elif "```" in output2:
                        start = output2.find("```") + 3
                        end = output2.find("```", start)
                        if end != -1:
                            output2 = output2[start:end].strip()
                    
                    try:
                        data = json.loads(output2)
                        sections = data.get('segmentation', {}).get('sections', [])
                        chunk_results.append({
                            'chunk_start': chunk_start,
                            'sections': sections
                        })
                        print(f"   âœ… {len(sections)} bÃ¶lÃ¼m Ã§Ä±karÄ±ldÄ± (retry ile)")
                    except:
                        # Son Ã§are: repair ile tekrar dene
                        repaired2 = _repair_json(output2)
                        try:
                            data = json.loads(repaired2)
                            sections = data.get('segmentation', {}).get('sections', [])
                            chunk_results.append({
                                'chunk_start': chunk_start,
                                'sections': sections
                            })
                            print(f"   âœ… {len(sections)} bÃ¶lÃ¼m Ã§Ä±karÄ±ldÄ± (retry + repair ile)")
                        except:
                            print(f"   âŒ Chunk {i} tamamen baÅŸarÄ±sÄ±z, atlanÄ±yor...")
                            print(f"   âš ï¸  Bu chunk'daki bÃ¶lÃ¼mler eksik kalacak!")
                except Exception as retry_error:
                    print(f"   âŒ Retry hatasÄ±: {retry_error}")
                    print(f"   âš ï¸  Chunk {i} atlanÄ±yor - bu chunk'daki bÃ¶lÃ¼mler eksik kalacak!")
        
        print()
    
    # SonuÃ§larÄ± birleÅŸtir
    print("ğŸ”— Chunk sonuÃ§larÄ± birleÅŸtiriliyor...")
    merged = merge_segmentations(chunk_results, text)
    
    # Validasyon
    sections = merged.get('segmentation', {}).get('sections', [])
    validation = validate_segmentation_result(sections, text)
    
    if not validation['valid']:
        print("âš ï¸  Validasyon hatalarÄ± bulundu:")
        for error in validation['errors']:
            print(f"   âŒ {error}")
    
    if validation['warnings']:
        print("â„¹ï¸  Validasyon uyarÄ±larÄ±:")
        for warning in validation['warnings'][:5]:  # Ä°lk 5 uyarÄ±yÄ± gÃ¶ster
            print(f"   âš ï¸  {warning}")
        if len(validation['warnings']) > 5:
            print(f"   ... ve {len(validation['warnings']) - 5} uyarÄ± daha")
    
    if validation['valid']:
        print("âœ… Validasyon baÅŸarÄ±lÄ±!")
    
    return json.dumps(merged, ensure_ascii=False, indent=2)


if __name__ == "__main__":
    # Test
    sample = "Introduction\nThis is a demo.\nMethodology\nWe did X.\nResults\n...\nConclusion\nDone." * 1000
    result = segment_text_chunked(sample)
    print(result[:500])

