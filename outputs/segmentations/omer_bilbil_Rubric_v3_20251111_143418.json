{
  "segmentation": {
    "sections": [
      {
        "section_id": "cover_1",
        "section_name": "COMP 200\nSummer Practice Report\nÖmer Bilbil\n042101089\n01.07.2025 – 31.07.2025, 21 workdays\nSubmitted: 29.09.2025\nMEF University\n_________________\nComputer Engineering Program",
        "content": "COMP 200\nSummer Practice Report\nÖmer Bilbil\n042101089\n01.07.2025 – 31.07.2025, 21 workdays\nSubmitted: 29.09.2025\nMEF University\n_________________\nComputer Engineering Program",
        "start_idx": 0,
        "end_idx": 180,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "executive_summary_1",
        "section_name": "Executive Summary",
        "content": "1\n\nExecutive Summary\nDuring my internship within the DevOps team at the Turkish Technology firm, I gained a\nlot of new information and also enhanced my technical capabilities significantly. Working\non this project gave me the avenue to use contemporary tools and technologies that\nstreamline software development as well as operations and save time. Throughout this\ninternship, I acquired hands-on experience with real practices and learned how companies\nefficiently employ programs and automate their business processes in order to maintain\nthem extremely reliable and operational.\nDocker was one of the main technologies that I learned throughout this internship. I used\nDocker to run applications inside containers. With this, I realized how Docker enables\napplications to run in a deterministic manner across environments, skipping most of the\ncompatibility issues. I also studied Kubernetes, which taught me how to scale and\nmaintain containerized applications in an efficient manner. This taught me about\norchestrating complex systems and how organizations maintain stability by adding more\nworkload.\nAlso, I gained handy experience deploying CI/CD (Continuous Integration and Continuous\nDeployment) pipelines using Jenkins. Through this, I witnessed first-hand how automation\naccelerates software release and minimizes errors in the deployment process. Also, I\ngained experience with OpenShift, an enterprise Kubernetes platform, which further\nadded to my skill set for deploying and maintaining applications on live production\nenvironments. Developing with OpenShift gave me practical knowledge of how enterprises\nhandle live deployments without sacrificing security, reliability, and scalability.\nApart from mastering all these basic DevOps tools, I also learned through a project\ninvolving artificial intelligence. Through the project, I created an AI-based solution that\ncalculates the most appropriate configuration for systems automatically. Through this, my\nsolution was intended to minimize manual effort, eliminate much of the intervention of\nhuman factors, and enhance overall efficiency in operation. This experience further\ncemented my understanding of how AI can be integrated into DevOps operations to\npromote innovation and automation in system maintenance.\nIn addition to gaining experience with several DevOps tools and technologies, this\ninternship greatly improved my problem-solving abilities, adaptability, and understanding\nof DevOps culture. I realized how collaboration, continuous improvement, and automation\nare the pillars of modern software development and operations. Overall, this internship\nwas extremely beneficial to my studies and professional life. The experience and\ntheoretical understanding I have obtained will certainly contribute to my future\nprofessional growth, research work, and technological contributions.",
        "start_idx": 181,
        "end_idx": 1969,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "contents_1",
        "section_name": "Contents",
        "content": "2\n\nContents\nExecutive Summary ................................................................................................................... 2\nContents ...................................................................................................................................... 3\n1. Company and Sector ......................................................................................................... 4\n1.1.Overview of the Company and Sector ............................................................................ 4\n1.2 Organization of the Company ......................................................................................... 5\na. Production/Service System ................................................................................................ 5\nb. Future of the Sector/Contemporary Issues ........................................................................ 7\n2. Summer Practice Description ............................................................................................ 8\na. Impact .............................................................................................................................. 10\nb. Team Work ...................................................................................................................... 10\nc. Life Long Learning .......................................................................................................... 10\n3. Conclusions ..................................................................................................................... 12\nReferences ................................................................................................................................ 13\nAppendix 1: Daily Activity Tables .......................................................................................... 14",
        "start_idx": 1970,
        "end_idx": 2690,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "impact_1",
        "section_name": "a. Impact",
        "content": "a. Impact .............................................................................................................................. 10",
        "start_idx": 2449,
        "end_idx": 2530,
        "level": 2,
        "parent_id": "conclusion_1"
      },
      {
        "section_id": "team_work_1",
        "section_name": "b. Team Work",
        "content": "b. Team Work ...................................................................................................................... 10",
        "start_idx": 2531,
        "end_idx": 2608,
        "level": 2,
        "parent_id": "conclusion_1"
      },
      {
        "section_id": "life_long_learning_1",
        "section_name": "c. Life Long Learning",
        "content": "c. Life Long Learning .......................................................................................................... 10",
        "start_idx": 2609,
        "end_idx": 2689,
        "level": 2,
        "parent_id": "conclusion_1"
      },
      {
        "section_id": "conclusions_1",
        "section_name": "3. Conclusions",
        "content": "3. Conclusions ..................................................................................................................... 12",
        "start_idx": 2690,
        "end_idx": 2761,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "company_sector_1",
        "section_name": "1. Company and Sector",
        "content": "3\n\n1. Company and Sector\nFigure 1.1\nTurkish Tecnology’s logo\nAddress: Halkalı Merkez Mah. Basın Ekspres Cad. No:3 Küçükçekmece / İstanbul\n1.1 Overview of the Company and Sector\nTurkish Airlines Technology and Informatics Inc. is a technology company that belongs 100%\nto Turkish Airlines. The company was founded in 2021 to support the digital transformation\nof the airline industry. Its main goal is to create new technology solutions that make air\ntravel easier, safer, and more efficient. The company works in many areas such as software\ndevelopment, IT infrastructure, data analytics, artificial intelligence, machine learning,\ncybersecurity, and financial technology solutions.\nOne of its important projects is TK Wallet, which helps passengers make faster payments\nand receive quick refunds. It also develops biometric systems that allow passengers to pass\nthrough airport checkpoints without contact, making travel faster and safer. Another big\nproject is the TROYA Passenger Service System, which is designed to improve all parts of the\npassenger journey according to international standards.\nThe company uses advanced tools like Red Hat OpenShift AI to build and run artificial\nintelligence models. With Artificial intelligence, it can save time, improve flight planning, and\nreduce costs. For example, thanks to Artificial intelligence and optimization systems, the\ncompany has saved around 10 million dollars in fuel and operations.\nInnovation is a very important part of Turkish Airlines Technology and Informatics Inc. The\ncompany supports new ideas from both employees and passengers. Through platforms like\nIDEAPORT (for employees) and IDEALIST (for passengers), many creative solutions are\ncollected and turned into real projects. It also works with startups through its terminal\nprogram, helping new businesses grow while benefiting from their innovative ideas.\nIn conclusion, Turkish Technology is not only a technology company for Turkish Airlines but\nalso a leader in aviation technology. It continues to work on new digital solutions, improve\nthe travel experience, and increase efficiency. With its focus on artificial intelligence,\n4",
        "start_idx": 2691,
        "end_idx": 4300,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "references_1",
        "section_name": "References",
        "content": "References ................................................................................................................................ 13",
        "start_idx": 2762,
        "end_idx": 2831,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "appendix_1",
        "section_name": "Appendix 1: Daily Activity Tables",
        "content": "Appendix 1: Daily Activity Tables .......................................................................................... 14",
        "start_idx": 2832,
        "end_idx": 2918,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "organization_1",
        "section_name": "1.2 Organization of the Company",
        "content": "innovation, and collaboration, the company is expected to play an important role in the\nfuture of global aviation. (Turkish Airlines Technology and Informatics Inc., 2024)\n1.2 Organization of the Company\nTurkish Technology has a large and well-structured organization to service its large portfolio\nof technological endeavors. The company has about 3500 engineers who are specialized in a\nnumber of technical areas such as software development, artificial intelligence, data\nanalytics, and cybersecurity. The engineers are distributed in 165 departments with specific\ntasks and projects being given to them.\nThe departments are under some directorates, which schedule the work of various teams\nand facilitate smooth operations of projects. Over them exist group directorates, which take\ncare of the larger strategic goals of the company and distribute resources among various\ndirectorates. At the top-level management, the company has a General Manager, who\nbeholds all of the company's operation and growth. He reports to the Chairman, who\nprovides overall guidance and ensures that the operations of the company tie in with the\nlong-term strategy of Turkish Airlines.\nThis organization in several levels helps Turkish Technology manage complicated technology\nprojects and promote innovation effectively. With a team of engineers and separate\nspecialized departments, Turkish Technology can handle different areas at once and easily\nrespond to new arising issues in the aviation technology sector.\nTEAM:\nMurat Yazar: DevOps Solutions Manager\nMuhammed Metin Uluyardımcı: Expert DevOps Engineer\nMuhammed Talha Şahin: Senior DevOps Engineer\nCemal İhsan Sofuoğlu: Senior DevOps Engineer\nAhmet Said Oyanık: Associate DevOps Engineer\nRengin Keten: Associate DevOps Engineer\nSelçuk Kınalı: Junior DevOps Engineer",
        "start_idx": 4230,
        "end_idx": 5801,
        "level": 2,
        "parent_id": "company_sector_1"
      },
      {
        "section_id": "production_service_1",
        "section_name": "a. Production/Service System",
        "content": "a. Production/Service System\nTurkish Technology is a technology company focused on designing and deploying digital\nsolutions for the aviation industry. The company adopts an Agile development approach\nthat enables fast response to customer needs. Regular feedback from aircraft maintenance\nexperts, operations managers, and IT specialists is integrated into the development process\nto guarantee continuous improvement.\nAt its core service infrastructure lies advanced software and data analysis infrastructure\naimed at enhancing maintenance processes, simplifying component tracking, and enhancing\noverall operating efficiency.\nThree stages constitute the company's service system:\nData Collection and Analysis:\n5\n\nData are gathered from aircraft maintenance processes, flight operational history, and\ncomponent usage reports. These datasets are analyzed for computing trends in system\nperformance as well as maintenance requirements.\nAlgorithm and Software Development:\nBased on the data that was processed, software modules are developed for managing\noptimal maintenance planning. Artificial intelligence and machine learning-based algorithms\nare utilized to predict failures and ease maintenance procedures.\nPersonalized Solution Delivery:\nThe systems developed are installed in the operations of the airlines. User-friendly\ninterfaces enable the maintenance teams to monitor processes in real time, and tailored\nreporting modules and alert systems provide tailored solutions for each client.\nBy this process, Turkish Technology reduces the cost of operation and minimizes downtime\nin flight, thereby making airline operations more efficient and dependable.",
        "start_idx": 5802,
        "end_idx": 7380,
        "level": 2,
        "parent_id": "company_sector_1"
      },
      {
        "section_id": "professional_ethical_1",
        "section_name": "b. Professional and Ethical Responsibilities of Engineers",
        "content": "b. Professional and Ethical Responsibilities of Engineers\nIn Turkish Technology, DevOps engineering teams are responsible for designing, deploying,\nand maintaining software systems supporting aviation operations to the highest levels of\nreliability, security, and efficiency. Their core professional tasks are:\nThe provision of reliable and stable deployment pipeline and minimizing downtime on\nsoftware upgrade and new feature release.\nEnsuring system security and regulatory compliance with aviation rules, cybersecurity best\npractices, and international standards.\nHarmonization of CI/CD processes for accelerated delivery cycles without compromising the\nquality or safety of software.\nPrinciples of Ethical Behavior applied by the engineering teams include:\nTransparency, in disclosing deployment processes, system updates, and potential operating\nimpacts to all stakeholders.\nInclusivity, through engineering deployment processes and tool sets that facilitate diverse\nteams within the organization, regardless of technical proficiency.\nResponsibility for operation and customer data management, whereby all data is\ntransmitted, stored, and processed securely and in accordance.\nWith such practices, Turkish Technology's DevOps teams have an efficient, secure, and\ncollaborative DevOps environment, which enables the delivery of quality technological\nsolutions that live up to the stringent operating requirements of the aviation sector.",
        "start_idx": 7381,
        "end_idx": 8879,
        "level": 2,
        "parent_id": "company_sector_1"
      },
      {
        "section_id": "future_sector_1",
        "section_name": "c. Future of the Sector / Contemporary Issues",
        "content": "c. Future of the Sector / Contemporary Issues\nI conducted an interview with my supervisor from the Turkish Technology office. He has\nbeen in aviation software for over a decade. He described that Turkish Technology\n6\nspecializes in digital solutions for aviation. The company builds systems for aircraft\nmaintenance, flight operations and data analysis. There are approximately 500 employees\ntoday and most of them are engineers and IT experts.\nHe clarified to me that the aviation sector is a bit different from other sectors like e-\ncommerce or social media. Safety and reliability come first in aviation. Any update in\nsoftware must be tested dozens of times before it comes to real airplanes. It retards the\nwork compared to other sectors, but it must be done. Conversely, an online store can launch\na new version of their site several times a day with minimal risk.\nMy supervisor also spoke about the future issues. One of the significant issues is having to\nmanage and process large data from airplane sensors. Planes produce enormous amounts of\ndata on every flight, and the data must be stored and analysed in a timely fashion. Another\nissue is that there is a shortage of engineers who know both aviation regulations and\nmodern software practices.\nLooking ahead, he believes that emerging technologies will reshape the industry over the\nnext five to ten years. Artificial intelligence and big data analytics will forecast maintenance\nproblems before they happen, which can be cost-effective and improve safety. Internet of\nThings will allow for around-the-clock monitoring of aircraft components. He also thinks 3D\nprinting can be used for some spare parts to reduce waiting time.\nHe compared aviation to the automotive industry. Cars already use many smart systems and\ncan update software over the air. Aviation will go down the same road but more slowly\nbecause of strict regulations.\nI gained from this interview an understanding that the future of aviation technology is very\nbright. Turkish Technology will need to adopt AI, IoT and other new technologies, but always\nwith careful consideration to safety and quality. For myself as a student, it shows how\nDevOps principles can be applied even in a life-critical field like aviation, but with special\ncare compared to faster-moving industries.",
        "start_idx": 8880,
        "end_idx": 10653,
        "level": 2,
        "parent_id": "company_sector_1"
      },
      {
        "section_id": "summer_practice_1",
        "section_name": "2. Summer Practice Description",
        "content": "7\n\n2. Summer Practice Description",
        "start_idx": 10654,
        "end_idx": 10704,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "activity_analysis_1",
        "section_name": "2. Summer Practice Description",
        "content": "2. Summer Practice Description\nI learned a lot about DevOps and modern software development during my summer\ninternship at Turkish Technology. My primary computer engineering tasks were to study\ncontinuous integration and continuous delivery (CI/CD) principles and how to build a pipeline.\nI observed how the company creates automated workflows to build, test, and deploy\napplications.\nI learned the company's own applications and observed how a pipeline is designed and\nexecuted. This explained to me the sense of linking various steps like code building, testing,\nand deploying to production without manual intervention.\nI also learned how to apply critical tools in the DevOps world: Docker, Kubernetes, and\nOpenShift. Docker is used to build containers such that applications deploy the same across\nall environments. Kubernetes and OpenShift are used to orchestrate these containers, scale\nthem, and run them reliably.\nThis work is directly related with my area of specialty in Computer Engineering. During\nuniversity I was instructed basic programming and software engineering, yet through work in\nthe company I gained to know how these technologies are used in real projects. I learned\nabout how DevOps operations reduce the development time and make it more secure.\nThe task was interesting and demanding. It was exciting to see how large teams collaborate\nand how automation removes human mistakes. At first, it was difficult to keep up with all the\nnew tools and commands, but my supervisor and coworkers helped me and I learned step by\nstep.\nBy this practice, I gained technical knowledge on CI/CD and container technologies and\nimproved my problem-solving ability. I also understood the importance of collaboration and\ncommunication in a working environment. Throughout this summer practice, I gained\nconfidence and motivation to study DevOps more in my career life.\n• Activity Analysis\nOne activity that was important to me was to review the code that is generated by other\nteams prior to being transmitted to the test environment. Our goal was to make sure that\nthe code does not contain any concealed configuration data or hard-coded credentials.\nUsually, when the developers push their code to Bitbucket, a trigger fires off a process that\nis used to deploy the code into the test environment. Before that process, our DevOps team\nreviews the code for security issues. With some help from another intern and working with\nthe team, I helped put in place a pipeline that would automatically identify possible hard-\ncoded credentials or configuration information. We used a tool called TK-GPT to aid in the\ndetection and to re-scan the code before proceeding with it.\nWe evaluated the performance of this pipeline by checking the number of risky files it could\ndetect and how fast it completed scanning. For example, we tracked the number of\nrepositories scanned within a day and how many problems were flagged. High detection\nrate and minimal false alarms were essential for excellent performance.\n8\n\nWhen we were verifying the data, we noticed some problems. Sometimes the pipeline\nreported false positives where the code contained words that looked like passwords but\nwere not credentials. Scanning extremely big repositories may also take more time than\nexpected. These issues show that we should improve the detection rules and maybe adjust\nthe scanning procedure.\nThis project closely relates to my Computer Engineering degree as it encapsulates\nprogramming, security, and automation. It was difficult and rewarding to create a system\nthat will secure sensitive information before it can be sent to the testing environment. I\nlearned how DevOps practices can directly make software more secure and of better quality\nin a professional setting through this project.\n9",
        "start_idx": 13407,
        "end_idx": 16620,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "impact_1",
        "section_name": "a. Impact",
        "content": "a. Impact\nEngineering solutions are able to reach the world in many ways and even a minimal change\nin a company can have far-reaching implications. At my internship, I helped to create a team\nthat produced an automated software security scan before code was allowed into the test\nenvironment. This is an example of how engineering affects global, economic,\nenvironmental and societal aspects all at once. Globally, modern software is utilized by\nnations and industries like aviation, banking and healthcare. Better security in one company\nsafeguards data that may be shared around the world and can encourage similar action in\nother companies. From a business point of view, leakage prevention of data and service\nfailures cost companies billions of dollars, legal penalties and reputations. An automated\ncheck system also reduces the level of manual checks, lowering the operational cost and\nenabling engineers to focus on something else. From an environmental point of view\nalthough the activity is computer software, automation leads to fewer paper reports, fewer\nunnecessary meetings and improved use of servers, saving power and carbon footprint of IT\nactivities. To society at large, protection of individual information and online service security\npromotes public confidence in technology; in areas like aviation, where dependability and\nsecurity are of most importance, secure software procedures also protect people's lives and\npersonal data. This experience illustrates that engineering activity in security and DevOps is\nnot only a technical endeavor but also a social responsibility, and that even a focused\nsoftware pipeline can have global, economic, environmental and societal implications.",
        "start_idx": 16620,
        "end_idx": 18503,
        "level": 2,
        "parent_id": "conclusion_1"
      },
      {
        "section_id": "conclusion_1",
        "section_name": "a. Impact",
        "content": "a. Impact\nEngineering solutions are able to reach the world in many ways and even a minimal change\nin a company can have far-reaching implications. At my internship, I helped to create a team\nthat produced an automated software security scan before code was allowed into the test\nenvironment. This is an example of how engineering affects global, economic,\nenvironmental and societal aspects all at once. Globally, modern software is utilized by\nnations and industries like aviation, banking and healthcare. Better security in one company\nsafeguards data that may be shared around the world and can encourage similar action in\nother companies. From a business point of view, leakage prevention of data and service\nfailures cost companies billions of dollars, legal penalties and reputations. An automated\ncheck system also reduces the level of manual checks, lowering the operational cost and\nenabling engineers to focus on something else. From an environmental point of view\nalthough the activity is computer software, automation leads to fewer paper reports, fewer\nunnecessary meetings and improved use of servers, saving power and carbon footprint of IT\nactivities. To society at large, protection of individual information and online service security\npromotes public confidence in technology; in areas like aviation, where dependability and\nsecurity are of most importance, secure software procedures also protect people's lives and\npersonal data. This experience illustrates that engineering activity in security and DevOps is\nnot only a technical endeavor but also a social responsibility, and that even a focused\nsoftware pipeline can have global, economic, environmental and societal implications.\nb. Team Work\nThe DevOps group was in daily meeting for the daily sprint meeting where every person\ndescribed what he did and what was coming next in short. These short meetings kept\neverything organized and allowed problems to be recognized early.\nOne such special event was a retrospective session. Members of the team met physically\nand openly shared good and bad points of the last sprint. Everybody had a chance to share\ngood stories, little complaints or improvement suggestions. This open talk created trust and\nenabled the team to prepare more effectively for the upcoming cycle.\nIn the security-check pipeline project, the working team consisted of a senior DevOps\nengineer, another intern, and myself. The senior engineer was responsible for technical\ndirection, provided best practices, and brushed aside complex obstacles. The other intern\nand I worked on writing and testing scripts, peer reviewing each other's work and learning\nnew tools.\nThe team was properly organized and the duties were defined. Planning tasks was left to the\nsenior engineer, there was friendly and open communication, and all the members\ncontributed to decision-making. As a result, work was finished on time and the pipeline\nproduced the expected results.\nThis project illustrated how good open communication, honest feedback and clearly\nspecified responsibilities can make an engineering project effective and enjoyable, and how\neffective team culture fosters learning and achievement.\nc. Life Long Learning\n10\n\nProfessional engineering practice requires a schedule of continuous learning and willingness\nto embrace new skills. During the internship, I clearly understood that learning at university\nis just a starting point and that there is always a need to look for new information and learn\nnew technologies.\nFor successful professional practice, it is vital to acquire additional technical competencies\nlike a thorough understanding of DevOps culture, CI/CD practices and container\ntechnologies like Docker, Kubernetes and OpenShift. These tools and practices keep\nchanging rapidly, so the ability to learn them continuously is required. Additionally, good\nsoft skills such as teamwork, communication, problem solving and time management are\nneeded since engineering work is almost always done in teams. Good attitude, curiosity and\nwillingness to ask questions are also applicable attitudes for a professional engineer.\nTo be able to analyze the activity and complete the project, more information were needed\nbeyond the content that I was exposed to in my classes. As an example, in order to create\nthe pipeline that scans for hardcoded credentials I had to learn about how a CI/CD pipeline\noperates in general, how you can take advantage of trigger mechanisms in Bitbucket, and\nhow security checks can be automated. I also had to learn about best practices in software\nsecurity and how DevOps teams integrate these checks before code is sent to a test\nenvironment.\nI managed to gather this relevant information through various means. One, I interviewed my\nsenior engineers and team members; what they said enlightened me on company practice\nand real usage. Two, I studied the official manuals of Docker, Kubernetes and OpenShift,\nwhich gave me decent technical information and examples. I also went through online blogs\nand viewed short training clips on CI/CD pipelines and security scanning. Finally, I looked\nthrough the company's internal reports and previous work to see how similar problems\nwere handled.\nThis experience was a lesson to me that lifelong learning is not merely taking classes or\nreading books. It is an active process of questioning, observing, learning and doing. By\ncombining university expertise with competencies gained in the workplace and by being\nopen-minded to autonomous learning, I am more prepared for a professional engineering\ncareer and for the future improvements in technology that will take place.\n11",
        "start_idx": 16620,
        "end_idx": 21201,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "team_work_1",
        "section_name": "b. Team Work",
        "content": "b. Team Work\nThe DevOps group was in daily meeting for the daily sprint meeting where every person\ndescribed what he did and what was coming next in short. These short meetings kept\neverything organized and allowed problems to be recognized early.\nOne such special event was a retrospective session. Members of the team met physically\nand openly shared good and bad points of the last sprint. Everybody had a chance to share\ngood stories, little complaints or improvement suggestions. This open talk created trust and\nenabled the team to prepare more effectively for the upcoming cycle.\nIn the security-check pipeline project, the working team consisted of a senior DevOps\nengineer, another intern, and myself. The senior engineer was responsible for technical\ndirection, provided best practices, and brushed aside complex obstacles. The other intern\nand I worked on writing and testing scripts, peer reviewing each other's work and learning\nnew tools.\nThe team was properly organized and the duties were defined. Planning tasks was left to the\nsenior engineer, there was friendly and open communication, and all the members\ncontributed to decision-making. As a result, work was finished on time and the pipeline\nproduced the expected results.\nThis project illustrated how good open communication, honest feedback and clearly\nspecified responsibilities can make an engineering project effective and enjoyable, and how\neffective team culture fosters learning and achievement.",
        "start_idx": 18503,
        "end_idx": 19988,
        "level": 2,
        "parent_id": "conclusion_1"
      },
      {
        "section_id": "self_directed_learning_1",
        "section_name": "c. Life Long Learning",
        "content": "c. Life Long Learning\n10\n\nProfessional engineering practice requires a schedule of continuous learning and willingness\nto embrace new skills. During the internship, I clearly understood that learning at university\nis just a starting point and that there is always a need to look for new information and learn\nnew technologies.\nFor successful professional practice, it is vital to acquire additional technical competencies\nlike a thorough understanding of DevOps culture, CI/CD practices and container\ntechnologies like Docker, Kubernetes and OpenShift. These tools and practices keep\nchanging rapidly, so the ability to learn them continuously is required. Additionally, good\nsoft skills such as teamwork, communication, problem solving and time management are\nneeded since engineering work is almost always done in teams. Good attitude, curiosity and\nwillingness to ask questions are also applicable attitudes for a professional engineer.\nTo be able to analyze the activity and complete the project, more information were needed\nbeyond the content that I was exposed to in my classes. As an example, in order to create\nthe pipeline that scans for hardcoded credentials I had to learn about how a CI/CD pipeline\noperates in general, how you can take advantage of trigger mechanisms in Bitbucket, and\nhow security checks can be automated. I also had to learn about best practices in software\nsecurity and how DevOps teams integrate these checks before code is sent to a test\nenvironment.\nI managed to gather this relevant information through various means. One, I interviewed my\nsenior engineers and team members; what they said enlightened me on company practice\nand real usage. Two, I studied the official manuals of Docker, Kubernetes and OpenShift,\nwhich gave me decent technical information and examples. I also went through online blogs\nand viewed short training clips on CI/CD pipelines and security scanning. Finally, I looked\nthrough the company's internal reports and previous work to see how similar problems\nwere handled.\nThis experience was a lesson to me that lifelong learning is not merely taking classes or\nreading books. It is an active process of questioning, observing, learning and doing. By\ncombining university expertise with competencies gained in the workplace and by being\nopen-minded to autonomous learning, I am more prepared for a professional engineering\ncareer and for the future improvements in technology that will take place.\n11",
        "start_idx": 19988,
        "end_idx": 21201,
        "level": 2,
        "parent_id": "conclusion_1"
      },
      {
        "section_id": "conclusion_2",
        "section_name": "3. Conclusions",
        "content": "3. Conclusions\nMy summer internship with Turkish Airlines Technology and Informatics Inc. provided a full\npicture of how modern DevOps practices are being adopted in a high-reliability, safety-\noriented industry such as aviation. I observed the way the company integrates innovation\nwith strict regulatory and security compliance to offer reliable technology solutions to\nairlines. Through exposure to tools like Docker, Kubernetes, and OpenShift, I enhanced my\ntechnical skills along with learning how CI/CD pipelines simplify development and maintain\nquality.\nParticipating in the security-oriented exercise to detect hard-coded credentials opened my\neyes to how DevOps is directly involved in protecting sensitive data and ensuring\ncompliance. It also depicted the importance of finding balance between automation and\ncareful human inspection to keep false positives low and continually improve processes.\nOverall, this experience not only better prepared me as a technical professional but also\ntaught me the value of teamwork, communication, and ethical responsibility in technology\nprojects on a large scale. I gained more confidence with the use of engineering principles in\nsolving real-world issues and had a clearer sense of how I can contribute to the\nadvancement of aviation technology using secure, efficient, and innovative DevOps\npractices.\n12",
        "start_idx": 21201,
        "end_idx": 22388,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "references_1",
        "section_name": "References",
        "content": "References\nTurkish Airlines Technology and Informatics Inc. (2024) About us. Available at:\nhttps://www.turkishairlines.com (Accessed: 29 September 2025).\n13",
        "start_idx": 22388,
        "end_idx": 22573,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "daily_activities_1",
        "section_name": "Daily Activity Tables",
        "content": "Daily Activity Tables\nDay 1 – Meeting the DevOps Team\nToday was my first day working in the company, and I was both excited and anxious. When I\narrived in the morning, I was met by my mentor in the lobby and brought to meet the\nDevOps team. The office was nicely lit and ventilated, with many desks and big monitors\nshowing dashboards and terminal windows with plenty of text. I didn't know much of what\nthere was on the screens at this stage, but it looked interesting and somehow mysterious.\nI interviewed the DevOps group individually. They were extremely friendly and told me what\nthey did. One was doing cloud infrastructure, one automation scripts, and one Kubernetes\nclusters. They also mentioned that they work very closely with software developers on a\nregular basis and sometimes work with security teams.\nMy mentor introduced me to what I will be working on during my internship briefly in the\nafternoon. The plan is to learn the basics of DevOps first, take courses on Docker and\nKubernetes, next learn CI/CD concepts, and finally integrate an AI-based project to scan\nrepository sites for sensitive information. I was relieved because this is exactly what I was\nanticipating and looking forward to - hands-on learning.\nAt night, I had a small coffee break with my team. They told me that communication within\nDevOps is equally important as technical skills. This made me think that I have to pay\nattention not just to tools but also to how people work. I left the workplace motivated for\nthe next day.\nDate Supervisor’s Name Signature\n01.07.2025 Muhammed Metin\nUluyardımcı\n14",
        "start_idx": 22573,
        "end_idx": 24173,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "daily_activities_1",
        "section_name": "Day 2 – Introduction to DevOps",
        "content": "Day 2 – Introduction to DevOps\nToday was my first ever daily stand-up meeting. The session was short but effective. Everyone\nin the team spoke about what they had done the day before, what they are going to do today,\nand if they had any problems. I discovered that this style of meeting is part of the Agile\napproach. It makes the team aware of everything and solves problems in good time.\nAfter meeting, my mentor gave me an introduction document to DevOps. I spent most of the\nmorning reading and took notes. I learned that DevOps is not a tool or technology—it is a\nculture and way of thinking. It is the unification of \"Development\" and \"Operations\" to deploy\nsoftware faster, more consistently, and with fewer bugs.\nI also learned a few of the essential DevOps practices: continuous integration, continuous\ndelivery, infrastructure as code, automated testing, and monitoring. These concepts sounded\nunfamiliar to me, but I realized their significance. For instance, if a company can test and\ndeploy their applications automatically, they can release updates much quicker compared to\na team that does everything manually.\nDuring the afternoon session, my mentor showed me some real-life scenarios. On his monitor,\nI saw a pipeline automatically outputting code once a developer had committed it to GitHub.\nIt was like magic—no deployment and compilation by hand, only automation. I started\nthinking about how such pipelines could make a team's life easier.\nBy the end of the day, I was convinced. Yesterday I had heard of the word \"DevOps\"; today I\nknow that it is a collection of culture, practices, and tools that help to deliver software in a\nbetter and faster way.\nDate Supervisor’s Name Signature\n02.07.2025 Muhammed Metin\nUluyardımcı\n15\n\nDay 3 – Deeper into DevOps Concepts\nMy mentor held a one-on-one learning session with me today. We sat in a meeting table,\nand he explained how DevOps changes the way businesses function. In the older models,\ndevelopers and operations people are separated. Developers write code and \"throw it over\nthe wall\" to operations, who then deploy it. This typically creates problems because the two\ngroups don't always understand the difficulties of the other. DevOps gets around this by\nhaving them work together from the start.\nHe also spoke about how automation is necessary. Without automation, teams spend too\nmuch time doing repetitive tasks like copying files, running tests manually, or restarting\nservers. Automation tools make these tasks happen by themselves, typically in a matter of\nseconds.\nLater in the afternoon, I watched a series of brief training videos on DevOps basics. The\nvideos covered subjects that included version control systems (Git), containerization, cloud\nplatforms, and monitoring tools. While it was too much to take in at once, I tried to take\nnotes and focus on the major ideas.\nBefore leaving the office, I helped my mentor with a small task: checking a log file to see if a\nscript ran successfully. It was a small contribution, but it made me feel like I was already part\nof the flow.\nDate Supervisor’s Name Signature\n03.07.2025 Muhammed Metin\nUluyardımcı\n16\n\nDay 4 – Discovering Containers\nToday I took my first deep dive into Docker. I had previously simply known that DevOps\nengineers required it but wasn't exactly sure why. My mentor described containers as small,\nportable boxes with all an application needs to run—its code, libraries, and even\nconfigurations. They can be moved anywhere and operate the same way no matter the\ncomputer or operating system unlike traditional installations.\nI had Docker running on my laptop in the morning. It was as if I opened a window to another\nuniverse. My mentor showed me how containers are so much lighter in comparison to\nvirtual machines and can be started in seconds. I was surprised that one laptop could have\nseveral different environments running at the same time without the laptop slowing down\ntoo much.\nBy the end of the day, I was already thinking about the idea of \"images,\" which are basically\nblueprints for containers. They can be downloaded from online repositories or created\nmanually. It made me realize that with Docker, you don't spend hours installing tools—you\nsimply use an image that's already ready to rock. At the end of the day, I was left wondering\nhow these containers communicate and interact with one another.\nDate Supervisor’s Name Signature\n04.07.2025 Muhammed Metin\nUluyardımcı\n17\n\nDay 5 – Seeing the Bigger Picture of Docker\nThis morning, I learned more about where containers are involved in the broader DevOps\nuniverse. Previously, the developers would always say to us, \"It works on my machine,\" and\nthat was never a guarantee that it would work somewhere else. With containers, that is\npractically eliminated because everything the app requires is encapsulated inside.\nMy mentor defined Docker images as snapshots of an application—ready to be \"brought to\nlife\" at any given time. Running, they are containers that are isolated from each other and\nfrom the underlying operating system. This is important because it shields applications from\nmessing with other applications.\nWe also talked about volumes, which enable containers to hold and exchange information\neven if they have been closed. This was a new concept to me, and it caused me to consider\nhow actual applications, such as databases, must secure their data.\nBy evening, I understood Docker is not a tool—Docker is a philosophy of thinking of software\nas being a self-contained, portable package. It's faster to deploy and easier to manage.\nDate Supervisor’s Name Signature\n07.07.2025 Muhammed Metin\nUluyardımcı\n18\n\nDay 6 – Building My First Custom Environment\nToday was creativity day. I learned how to create my own \"recipe\" for a container so that it\nstarts off exactly the way I require. My mentor told me that this is one of the most helpful\nabilities in DevOps: creating environments that are consistent and reproducible for everyone\non the team.\nWe discussed how operating systems handle containers. Containers, in contrast to virtual\nmachines, share the kernel of the host system and are thus lighter in weight. This enables\nme to run many containers on one machine without squandering too many resources.\nThe idea of building my own image was interesting. It was similar to designing a small\nworkspace, filling it with what I need, and shutting the door so nothing else could enter.\nHaving viewed it up and running, it was sort of magical—like creating a mini computer inside\nmy computer.\nBy the end of the day, it was simple to visualize how containers revolutionize the manner in\nwhich teams deploy and build software. They render it faster, cleaner, and much more\ndependable.\nDate Supervisor’s Name Signature\n08.07.2025 Muhammed Metin\nUluyardımcı\n19\n\nDay 7 – Orchestrating with Docker Compose\nToday I discovered that sometimes one container is not enough. Real-world applications are\ncomposed of many pieces a web server, a database, maybe even a cache system. My mentor\nintroduced me to a tool that can create and manage all of these containers in one\ncommand, Docker Compose.\nIn the morning, I saw how this utility can describe an entire multi-container configuration in\none file. It was like copying down a recipe, but instead of one dish, you prepare a full meal\nmade up of different courses. With a single effortless command, all the containers can be\nspun up and networked with each other as if they were all in the same kitchen.\nWe talked about why this is beneficial for development and testing. Instead of booting up\neach piece manually, you can boot up the entire setup in seconds. By the afternoon, I was\ngrateful to see how Docker Compose increases efficiency and eliminates mistakes, especially\nwhen working with multiple people. It was my initial exposure to \"orchestration,\" the idea of\nmanaging lots of moving pieces simultaneously.\nDate Supervisor’s Name Signature\n09.07.2025 Muhammed Metin\nUluyardımcı\n20\n\nDay 8 – First Impressions of Kubernetes\nToday was just the start of something much bigger—Kubernetes. My mentor called it\n\"the conductor of the container orchestra.\" Whereas Docker runs and controls\nindividual containers, Kubernetes manages multiple of them across multiple machines\nso that all goes well.\nI discovered clusters in the morning, those being groups of machines (or nodes) that\ncan be operated together. Inside these clusters, Kubernetes runs \"pods,\" which might\nbe said to be small groups of containers. It decides where a pod should live, restarts\nthem when they fail, and even duplicates them if the app needs to serve more users.\nAt first, it was intimidating. The diagrams seemed complicated, and there were so\nmany new acronyms—namespaces, deployments, services. But I was reminded by my\nmentor that Kubernetes is like learning a new city. At first, you know the main streets,\nbut eventually, you start learning the smaller streets.\nAt the end of the day, I came to the conclusion that Kubernetes is not so much about\nexecuting containers as it is about keeping them healthy, networked, and ready to\nscale as necessary.\nDate Supervisor’s Name Signature\n10.07.2025 Muhammed Metin\nUluyardımcı\n21\n\nDay 9\nToday I was able to witness firsthand how Kubernetes makes applications\ndependable. My mentor explained to me that pods are the most basic Kubernetes has,\neach housing a single container or multiple ones. When a container in a pod crashes,\nKubernetes does not panic—it simply starts a new one.\nWe discussed why this matters so much in the real world. If a site crashes, users can\nleave and never come back. Kubernetes prevents this from occurring by checking on\nevery pod like an overbearing parent. It can even replace pods with new ones without\nshutting down the whole system, so users won't even notice there was a disruption.\nDuring the afternoon session, I saw a demonstration of Kubernetes deploying pods to\nseveral machines in the cluster. This makes sure that if any machine goes down, the\napplication remains operational on others. It was nearly a safety net for containers,\nand they would never be shut down.\nBy the end of the day, I was more at ease with the underpinnings, though I still have a\nlot to learn. Kubernetes felt like a complex but powerful friend I was only just getting\nto know.\nDate Supervisor’s Name Signature\n11.07.2025 Muhammed Metin\nUluyardımcı\n22\n\nDay 10 – Services and Networking in Kubernetes\nMy mentor told me early this morning that we would be covering how containers in\nKubernetes can find and talk to each other. He explained that Kubernetes pods can be\nmoved from one node to another, and their IP addresses could change as well. This\ncomplicates the ability to access a pod directly. That is why Kubernetes has something\nknown as a \"Service.\" A Service is like a constant point of contact. Even though the pod\nchanges, the Service stays the same, so communication never gets interrupted.\nI learned different Service types. \"ClusterIP\" is used for communication within the cluster,\n\"NodePort\" publishes the application outside the cluster through a port on the node, and\n\"LoadBalancer\" auto-provisions an external IP so people can access the application through\nthe internet. It was similar to learning how to offer addresses to houses inside a city so\npeople know where to go.\nMy mentor performed an example in the afternoon. He used two pods with a small web\napplication. Afterward, he deleted one of the pods on purpose. I was thinking that the\napplication would crash, but it didn't. The Service forwarded the traffic to the second pod\nwithout any issues. This showed me how Kubernetes was designed to be reliable.\nAt the end of the day, I felt more confident. Networking in Kubernetes was still convoluted,\nbut I started to understand the \"why\" of it. It's not just plugging containers together—it's\nabout making applications endure no matter what happens in the background.\nDate Supervisor’s Name Signature\n16.07.2025 Muhammed Metin\nUluyardımcı\n23\n\nDay 11 – Deployments and Scaling in Kubernetes\nToday was more about applications in a smarter way. My mentor demonstrated Kubernetes\nDeployments for me. A Deployment is like a manager that maintains the proper number of\nrunning pods and that they should be running with the proper version of the application.\nIn the morning, we rolled out a sample web application and formed a Deployment. It\noriginally had two pods. We later scaled it to five pods using a single command. I was\ncapable of seeing the extra pods being spun up in mere seconds. My mentor explained that\nthis is called \"horizontal scaling.\" It is used when users are greater and the application needs\nmore resources to handle the traffic.\nWe also learned to roll out updates. Instead of restarting all the pods at the same time,\nKubernetes replaces them one by one. This is an \"update rolling\" and guarantees that the\napplication is never unavailable. Late in the afternoon, we practiced it by changing the\nbackground color of the web application. I committed the new code, and the Deployment\nrolled out the update to each pod until they all showed the new color—not a moment of\ndowntime.\nThis day showed me that Kubernetes is more than just a tool to run containers, but it's also a\nway to run them in an optimal fashion, with the minimum amount of impact to users.\nDate Supervisor’s Name Signature\n17.07.2025 Muhammed Metin\nUluyardımcı\n24",
        "start_idx": 25992,
        "end_idx": 31967,
        "level": 2,
        "parent_id": "daily_activities_1"
      },
      {
        "section_id": "activity_analysis_1",
        "section_name": "Day 12 – Introduction to CI/CD",
        "content": "Day 12 – Introduction to CI/CD\nI started learning about CI/CD—Continuous Integration and Continuous Deployment today. I\nwas informed by my mentor that it is one of the major principles in DevOps because it\nautomates software delivery.\nHe explained Continuous Integration this morning. I.e., the developers push their code\ncontinuously to a shared repository, and upon each push, an automated build runs that\ncompiles the code and executes tests. If the build or tests fail, the developers fix it\nimmediately before the problem spirals out of control.\nIn the afternoon, we had a glimpse of Continuous Deployment. It is where the code that has\nbeen tested gets deployed to production or staging automatically without any human\ninteraction. My mentor showed me a sample pipeline in GitLab. It had steps like \"build,\"\n\"test,\" and \"deploy.\" When a developer pushed the code, the pipeline created a Docker\nimage, tested it, and deployed to Kubernetes. Everything was done automatically without\nany human being interacting with the server.\nI was amazed at how much time and effort CI/CD saves. Instead of doing everything\nmanually and risking mistakes, the system ensures that everything is done in the correct\nsequence, every time.\nDate Supervisor’s Name Signature\n18.07.2025 Muhammed Metin\nUluyardımcı\n25\n\nDay 13 – Building My First CI Pipeline\nToday I created my first CI pipeline on GitLab. At first, it was extremely simple—it contained\nonly a single job that printed \"Hello World.\" But it made me happy because I was able to see\nthe job get started, run, and finish successfully.\nMy mentor explained to me that pipelines are made of \"stages\" and \"jobs.\" Stages run one\nafter the other, and jobs within a stage can run in parallel. If one job fails, the pipeline stops.\nThis ensures that rotten code never gets to the next step.\nDuring the afternoon, we added a build phase that created a Docker image and pushed it\ninto a local registry. It was sweet to behold my very own image in the registry. I could\npicture in my mind that one day I would push real application images here and deploy it into\nKubernetes.\nUltimately, I realized that CI/CD is like having a factory assembly line. Once you set it up, it\nwill naturally provide you with consistent and quality outputs.\nDate Supervisor’s Name Signature\n21.07.2025 Muhammed Metin\nUluyardımcı\n26\n\nDay 14 – Automated Testing in CI/CD\nThis morning, I learned how to integrate automated tests into my pipeline. My mentor said\nwithout tests, automation is risky—you could be rolling out broken code and not even know\nit.\nWe started with simple unit tests for a small Python program. I added a \"test\" stage in the\npipeline. The pipeline failed before deploying if the test failed. This safeguard is needed in\nactual projects since it prevents faults from reaching the user.\nWe intentionally introduced a bug in the code during the afternoon to see what would\noccur. The pipeline ran, the test failed, and deployment was prevented. It was pleasant to\nknow that this system would protect the production from bad updates.\nDate Supervisor’s Name Signature\n22.07.2025 Muhammed Metin\nUluyardımcı\n27\n\nDay 15 – Continuous Deployment to Kubernetes\nIt was one of the most fascinating days of my internship. We merged the CI/CD pipeline with\nKubernetes. Now, whenever I commit code, it automatically gets built, tested, and deployed\nto the cluster.\nWe used kubectl commands in the deploy stage of the pipeline to deploy the new Docker\nimage to the Kubernetes Deployment. I committed a change to the app's HTML, and within a\nfew minutes, I could see the new version running in the browser.\nLater in the afternoon, I attempted to make small adjustments to see how fast and stable\nthe system was. Everything went smoothly. That was when I truly understood the power of\nDevOps automation.\nDate Supervisor’s Name Signature\n23.07.2025 Muhammed Metin\nUluyardımcı\n28\n\nDay 16 – Starting the AI Config Project\nI began the final and most unique part of my internship today—integrating AI into DevOps\nfor security. My mentor explained how developers sometimes accidentally put passwords,\nAPI keys, or tokens into code. This is \"hardcoding credentials,\" and it's problematic since if\nthe code is exposed, attackers can steal this data.\nWe were to use an AI model to search repositories and detect sensitive information in a\nclick. Morning was spent detailing how the model operates and what amount of data it\nneeds. Afternoon was spent helping to prepare a list of examples: some with real-looking\nbut fake credentials, and others with clean code. These would be used to train and test the\nAI.\nI began the final and most unique part of my internship today—integrating AI into DevOps\nfor security. My mentor explained how developers sometimes accidentally put passwords,\nAPI keys, or tokens into code. This is \"hardcoding credentials,\" and it's problematic since if\nthe code is exposed, attackers can steal this data.\nWe were to use an AI model to search repositories and detect sensitive information in a\nclick. Morning was spent detailing how the model operates and what amount of data it\nneeds. Afternoon was spent helping to prepare a list of examples: some with real-looking\nbut fake credentials, and others with clean code. These would be used to train and test the\nAI.\nDate Supervisor’s Name Signature\n24.07.2025 Muhammed Metin\nUluyardımcı\n29\n\nDay 17 – Data Collection for the AI Project\nToday I spent most of the time on collecting and preparing code examples for the AI\nmodel. We needed samples of different types of credentials: passwords, API tokens, and\ndatabase connection strings.\nI also learned of \"false positives,\" when the AI thinks there is a secret but it is not one.\nFor example, the word \"password\" used in a remark might be interpreted as an actual\npassword. We had to introduce such examples so that the AI could be taught to make no\nmistakes.\nI had a much better understanding of how important data is in AI projects by evening.\nEven the best model would not work without good data.\nDate Supervisor’s Name Signature\n25.07.2025 Muhammed Metin\nUluyardımcı\n30\n\nDay 18 – Testing the AI Model\nWe experimented with a pre-trained secret-detecting AI today. My mentor ran it on a few\ntest repositories, and it identified the spoof passwords we had placed there for testing.\nThere were some false positives, however. That led to some debate about tuning the model\nand making it more precise. In the afternoon, I helped make more diverse examples to train\non so that the AI would learn better.\nIt was interesting to see how AI may help in DevOps by incorporating security testing into\nthe development process.\nDate Supervisor’s Name Signature\n28.07.2025 Muhammed Metin\nUluyardımcı\n31\n\nDay 19 – Integrating AI into CI/CD\nToday morning, I along with my mentor initiated the most important step of the AI security\nproject—embedding the AI scanning process inside the CI/CD pipeline. The goal was simple:\nevery time a developer pushes new code to the repository, the pipeline would scan for\nhardcoded credentials automatically before deploying the code.\nWe started by adding a new stage in the GitLab pipeline configuration. We named it\n\"security-scan.\" In this stage, the AI tool would run and scan all the repository files for\nsensitive information like passwords, API keys, and tokens. My mentor informed me that\nthis is a critical step because sometimes developers might commit mistakes and accidentally\nplace secrets in their code. With this ongoing check, these mistakes can be caught right\naway.\nLate morning, our first test. I pushed some test code with a dummy API key baked inside it.\nThe pipeline rolled along just as usual: it built the Docker image, ran the automated tests,\nand deployed to the temporary environment. But just as the deployment was about to\nfinish, the AI scan detected the dummy key and halted the pipeline in its tracks. There was a\nwarning message that appeared in pipeline logs indicating: \"Possible hardcoded credential\nfound in app/config.js – Please review and remove.\"\nIt was amazing to watch it execute live. It validated the system to perform as intended.\nDuring the afternoon, we executed the test with good code to verify the pipeline would\nexecute smoothly without errors if secrets were not found. It processed without a problem.\nBy day's end, I realized just how powerful this integration was. We had combined DevOps,\nsecurity, and AI into a single automated pipeline. It was no longer simply about moving\napplications out the door fast—it was about having them be secure from day one as well.\nThis was my first time seeing such a powerful example of \"DevSecOps\" in action.\nDate Supervisor’s Name Signature\n29.07.2025 Muhammed Metin\nUluyardımcı\n32\n\nDay 20 – Final Testing and Documentation\nToday was all about making sure our system worked perfectly and could be maintained in\nthe future. My mentor told me that in professional environments, testing and\ndocumentation are just as important as building the system itself.\nIn the morning, we did a full end-to-end test of the pipeline. We followed the entire process:\n1. Push new code to the repository.\n2. Pipeline starts automatically.\n3. Build stage creates a new Docker image.\n4. Test stage runs automated unit tests.\n5. Deploy stage updates the application on Kubernetes.\n6. Security-scan stage uses the AI tool to check for hardcoded credentials.\nWe tried multiple scenarios—code with fake credentials, code with no credentials, and code\nwith tricky patterns that might confuse the AI. The system responded correctly every time. If\nthe AI found something, it stopped the pipeline and sent a clear warning. If not, it completed\nthe deployment without any issues.\nAfter confirming that everything was working, I started writing documentation. My mentor\nsaid this would help other team members understand how the pipeline works and how to\nimprove it in the future. I wrote about:\n• The purpose of each stage in the pipeline.\n• How the AI scanning tool works.\n• How to train the AI with new data.\n• Common problems and how to fix them.\nIt took me most of the afternoon to complete the documentation, but it was worth it. I knew\nthat when my internship ended, the project would still be useful for the team.\nBy the end of the day, I felt proud because we had created a complete, secure, and well-\ndocumented system that could be used in real production environments.\nDate Supervisor’s Name Signature\n30.07.2025 Muhammed Metin\nUluyardımcı\n33\n\nDay 21 – Last Day and Reflection\nToday was my final day at Turkish Airlines' DevOps team. I was sad and at the same time\nhappy. Happy due to the fact that I had learned a lot of things in these three weeks, and sad\nbecause I would miss working with such a talented and kind-hearted team.\nI prepared for my final presentation in the morning. I wanted to explain thoroughly\neverything that I learned and the effort that I put in. My slides were:\nWeek 1: Introduction to DevOps, learning about Docker, and creating custom container\nimages.\nWeek 2: Learning Kubernetes, creating deployments, scaling apps, and creating CI/CD\npipelines.\nWeek 3: The AI security project—data collection, testing the AI model, and implementing it\ninto the pipeline.\nAs it was presentation time, I was anxious, but the team and my mentor just smiled and\nassured me. I took them through the process of how the pipeline worked from start to\nfinish, and I showed them a live scan of the AI scanning for a fake credential. The team was\nimpressed and praised me on it being a great addition to their security process.\nMy mentor offered me feedback after the presentation. He said that I had actually made a\nsignificant input to the department and that my greatest assets were my curiosity and\nwillingness to learn. I felt overwhelmingly proud after hearing that.\nIn the night, I took a while to bid farewell to each member of the team. We talked about\nfunny moments in the office, moments I was mistaken for Kubernetes commands, and the\nsmall victories—like when my very first pipeline executed successfully.\nAs I left the office for the last time, I was surprised at how much I had picked up in three\nweeks. Initially, I knew very little about DevOps. Now, I had hands-on experience with\nDocker, Kubernetes, CI/CD, and even using AI to enhance a security process. I was not just\nleaving with technical skills, but also an awareness of how collaboration, communication,\nand documentation are important in the real world.\nI was grateful for the opportunity, and I promised to keep learning and improving day by\nday.\nDate Supervisor’s Name Signature\n31.07.2025 Muhammed Metin\nUluyardımcı\n34",
        "start_idx": 39398,
        "end_idx": 45848,
        "level": 2,
        "parent_id": "daily_activities_1"
      },
      {
        "section_id": "internship_documents_1",
        "section_name": "Internship Documents",
        "content": "Internship Documents\nCHECKLIST\nCOMP 200 Engineering Practice\nMEF University\n2025-2026\nFor the completeness of your internship report submission, you are held responsible to\ncheck the submission of the following items. The marked final checklist should be\nincluded to your submitted internship report as the last page.\n✓ Internship Application and Acceptance Form signed/stamped?\n✓ Internship Report\n✓ Student Evaluation Survey\n✓ Internship Evaluation Form enclosed/sealed\n✓ Bottom of Internship Report pages signed?\n✓ Additional material (such as source codes) attached to the submitted report\n✓ Internship Report bound and plastic-covered?\n✓ Internship Report and all accompanying material submitted in an envelope.\n✓ Hereby, I accept liability for the accuracy and integrity of the submitted contents.\nStudent Name: Ömer BİLBİL\nDate: 29.08.2025\nSignature\n35",
        "start_idx": 45850,
        "end_idx": 46539,
        "level": 1,
        "parent_id": null
      }
    ]
  },
  "source_metadata": {
    "total_length": 53670,
    "extraction_timestamp": "2025-11-11T14:34:18.025085",
    "chunked": true,
    "chunk_count": 4
  }
}