{
  "segmentation": {
    "sections": [
      {
        "section_id": "cover_1",
        "section_name": "COMP 399\nSummer Practice Report\nÖmer Bilbil\n042101089\n04.08.2025 – 29.08.2025, 20 workdays\nSubmitted: 29.09.2025\nMEF University\n_________________\nComputer Engineering Program",
        "content": "COMP 399\nSummer Practice Report\nÖmer Bilbil\n042101089\n04.08.2025 – 29.08.2025, 20 workdays\nSubmitted: 29.09.2025\nMEF University\n_________________\nComputer Engineering Pro",
        "start_idx": 0,
        "end_idx": 170,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "executive_summary_1",
        "section_name": "Executive Summary",
        "content": "gram\n\nExecutive Summary\nThis is my internship report describing the work I did under the AI Innovators program in my\ninternship at Microsoft. Microsoft is a global technology firm that builds software, services,\ndevices, and cloud solutions. Its main engineering work involves building the Windows\noperating system, Microsoft 365 applications, and the Azure cloud solution. Microsoft invests\nheavily in research, especially in artificial intelligence (AI). The AI Innovators project focuses\non using innovative AI technologies to solve actual problems. The project that I contributed\nto was focused towards this goal by using AI for improving the stock price prediction systems\nand helping the investors make the right choice.\nThe main aim of my internship was to create and deploy a Stock Price Prediction System using\ncutting-edge machine learning techniques. It was an end-to-end project of creating a tool\nusing several methods to analyze the historical financial data and forecast future stock prices.\nI first learned about the working mechanism of stock markets and the financial time series\ndata structure. Then I utilized this knowledge in developing a multi-step prediction pipeline.\nFirst, I built a data preprocessing module that collects and cleans large amounts of historical\nstock prices and significant financial metrics. Much of my effort went into designing a deep\nlearning model using PyTorch to be able to learn complex patterns and trends. I trained\nseveral models on a big real world dataset with millions of data points. On the final stage, I\nhad combined all parts of the project into a master engine that processes the preprocessing\nquickly on the first step and then sends the cleaned data to the trained PyTorch model to\nproduce accurate predictions. In order to make the system easily usable, I created two\ninterfaces: a command-line tool for advanced users and power users for automation, and a\nweb interface showing the prediction results in an understandable format and keeping a\nhistory of past forecasts.\nAs I started the internship, I tried to apply whatever I learned from my minor at university in\nterms of machine learning and software engineering to a real project and gain experience in\nfinancial AI solutions. I was also interested to find out how a full-fledged AI project is designed\nand run in a big tech company. The final product turned out much more than I expected. I\nbuilt a working and annotated application that utilizes smart design and multiple levels of\nanalysis, much like actual financial forecasting programs.\nI learned numerous technical skills through this projec",
        "start_idx": 170,
        "end_idx": 2790,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "contents_1",
        "section_name": "Contents",
        "content": "t, such as cleaning and preprocessing\ntime series data, training deep learning models using PyTorch, backend programming, and\nbuilding a frontend interface with Python. I also learned about handling a big project, the\nsignificance of possessing good data pipelines, and how to debug and correct issues in\nsoftware. I got to lead the project from the start to the completion, and it helped me build my\nconfidence and provided me with a keen interest in working in AI-driven financial forecasting\nin the future.\n2\n\nContents\nExecutive Summary ................................................................................................................... 2\nContents ...................................................................................................................................... 3\n1. Company and Sector ......................................................................................................... 4\na. Overview of the Company and Sector ............................................................................... 4\nb. Organization of the Company ........................................................................................... 5\nc. Production/Service System ................................................................................................ 6\nd. Professional and Ethical Responsibilities of Engineers .................................................... 8\n2. Summer Practice Description .......................................................................................... 10\na. Impact .............................................................................................................................. 16\nb. Team Work ...................................................................................................................... 17\nc. Life Long Learning .......................................................................................................... 17\n3. Conclusions ..................................................................................................................... 19\nReferences ................................................................................................................................ 20\nAppendix 1: Daily Activity Tables .......................................................................................... 21\n3\n\n",
        "start_idx": 2790,
        "end_idx": 5152,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "company_sector_1",
        "section_name": "1. Company and Sector",
        "content": "",
        "start_idx": 5152,
        "end_idx": 4700,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "organization_1",
        "section_name": "b. Organization of the Company",
        "content": "...................................... 17\n3. Conclusions ..................................................................................................................... 19\nReferences ................................................................................................................................ 20\nAppendix 1: Daily Activity Tables .......................................................................................... 21\n3\n\n1. Company and Sector\na. Overview of the Company and Sector\nCompany: Microsoft Corporation\nAddress: Microsoft Türkiye, Bellevue Residences, Levent Mahallesi, Aydın Sokak No:7,\n34340,\nBeşiktaş, İstanbul, Türkiye\nWebsites: https://www.microsoft.com (Global) | https://www.microsoft.com/tr-tr/ (Turkey)\nMicrosoft Corporation has a long history, since 1975, when Bill Gates and Paul Allen founded\nthe company to develop software to enable early personal computers to work. The company\nhas established itself as a software powerhouse with its launch of the MS-DOS operating\nsystem and the later Microsoft Windows operating system as its platform for PCs. Since its\ninception, Microsoft has grown significantly and expanded well beyond the world of PC\nsoftware. Today, Microsoft is a publicly traded company on the NASDAQ (MSFT), and it is a\nglobal leader in the high tech space. The vast majority of its activities are run from clean,\nindoor office buildings. In Turkey, the country's Microsoft headquarters is located in\nIstanbul, where they employ support staff, and primarily professional staff with expertise in\nengineering, sales, marketing, and support issues. There are over 220,000 employees\nglobally in Microsoft (Microsoft, 2024).\nMicrosoft works in a high technology sector space cal",
        "start_idx": 4700,
        "end_idx": 6441,
        "level": 2,
        "parent_id": "company_sector_1"
      },
      {
        "section_id": "production_service_1",
        "section_name": "c. Production/Service System",
        "content": "led Information and Communication\nTechnology (ICT) which is fundamentally important to the global economy for growth and\ninnovation (IDC, 2023). In this sector, the company's main purpose is to empower every\nperson and every organization on the planet to achieve more. Microsoft has a large market\nshare in key areas such as desktop operating systems (Windows) and cloud computing\n(Azure), and competes with a number of global technology companies like Google, Amazon,\nand Apple (Gartner, 2023). Microsoft has customer segments that include individual\nconsumers, large international corporations, and government entities both in Turkey and\nworldwide. Most of the work completed at Microsoft Turkey relates to digital\ntransformation across Turkey. The Microsoft Turkey local office works with Turkish\ncompanies to help them modernize by imp",
        "start_idx": 6441,
        "end_idx": 7280,
        "level": 2,
        "parent_id": "company_sector_1"
      },
      {
        "section_id": "company_sector_4",
        "section_name": "Performance of the system is quantified by such metrics as deployment velocity and service\navailability, which is formally assured by Service Level Agreements (SLAs) that guarantee\nuptimes frequently over 99.9%. This whole service provision system is underpinned with\nadvanced planning and forecasting. Microsoft does not predict the sale of individual services\nbut the aggregate demand in various regions in computing power. This information, which is\ngrounded on the past use and market trends, is very essential in planning on where and\nwhen to construct new data centers to address future customer demands.\nMoreover, the company has a special kind of inventory — the unused capacity of servers\nand storage of its data centers. This is a key balancing act in managing this digital inventory\nbetween not having enough capacity to meet unexpected customer demand and the cost of\n7\nhaving idle hardware. All this is possible through a huge physical logistics effort that controls\nthe worldwide supply chain of server hardware, including the procurement and shipment of\nhardware, its installation and its eventual decommissioning. This is a complicated backend\nstructure that is the real physical infrastructure that enables Microsoft to provide the global\ncloud services it does.",
        "content": "lementing cloud technologies and advanced\nsoftware solutions (Symantec, 2023). Microsoft creates local product versions and engages\nspecially in supporting the Turkish market and actively supports the local startup ecosystem\nthrough various technology and mentorship programs.\nFigure 1. Microsoft Logo\n4\n\nb. Organization of the Company\nMicrosoft Turkey is a subsidiary of Microsoft Corporation and possesses a matrix\nis an organizational structure with both functional as well as project-based methods. This\nhybrid structure allows the firm to achieve operational effectiveness in day-to-day activities\nwhile remaining flexible enough to keep up with quick-changing technology markets and\ncustomer needs.\nFigure 2. Microsoft Organizational Structure\nMajor Departments and Their Responsibilities\nSales and Marketing Division\nThis department is the main source of income. The Enterprise Sales Team targets large\ncorporations and government institutions with the Azure cloud services, Office 365, and\nWindows enterprise products. The SMB Sales Team focuses on small and medium\nbusinesses with solutions tailored to them. The Consumer and Device Sales team oversees\nretail relationships in the Surface devices, Xbox consoles and consumer software. The\nMarketing and Communications Team creates locally targeted campaigns and aligns with the\nworld Microsoft strategies.\nTechnical Division\nThe Technical Division is the source of customer support and expertise. The Solution\nArchitecture Team creates complex technology solutions to enterprise customers. The\nCustomer Success Team sustains the relationships with the clients and offers optimization\nadvice. Technical Support Services provides multi level customer support, and Cloud and\nInfrastructure Services team focuses on the implementation of Azure and cloud\nmanagement.\nPartner and Ecosystem Division\nThis department deals with the network of business partners of Microsoft Turkey. The\nPartner Development Team finds and develops new partnerships, training and equipping\nthem. ISV Relations collaborates with software companies to make sure that there is\n5\n\nintegration of platforms. Channel Partner Management is in charge of managing current\npartner performance and development.\nSupport Departments\nThe Human Resources deals with the recruitment, development of employees, and culture\nof the workplace. Finance and Operations is in charge of budgeting, financial reporting, and\nadherence to the Turkish regulations and Microsoft policies. Legal and Compliance makes\nsure that the business operations are in line with the local laws and regulations on data\nprotection.\nHierarchical Relationships and Structure Type\nMicrosoft Turkey operates with an organic structure characterized by:\n• Flexible Hierarchy: There is a proper reporting relationship but all levels are\nexpected to work cross functionally. The staff members also have frequent\ninteractions with other workers in other departments to resolve customer issues.\n• Decentralized Decision Making: The department heads enjoy a lot of autonomy in\ntheir areas of expertise and there is quicker response to the needs of customers and\nchanges in the market.\n• Project Based Collaboration: Employees are also allowed to work on cross\ndepartmental projects with the matrix structure allowing them to work on the main\nconcern. One example is that solution architects collaborate with sales, marketing\nand partner teams on large implementations.\n• Adaptive Communication: There is both vertical and horizontal flow of information\nthrough reporting channels and across departments through routine meetings and\ndigital tools as well as informal channels.\nc. Production/Service System\nMicrosoft is a technology firm that mainly manufactures software and offers digital services.\nMicrosoft does not have a traditional factory-based production system that produces\nphysical products, but a complicated system of software development, service management,\nand customer delivery. It is a system that develops, sustains and facilitates digital products\nsuch as Microsoft Windows, Office 365 and its cloud platform, Azure.\nThe primary objective of such a system is to provide value to customers with the help of\ndependable and innovative technological solutions. Microsoft has various departments\nwhich collaborate in an innovation, sales, and support cycle.\nBelow is a chart that describes the relationships between the major components of\nMicrosoft's service system.\n6\n\nFigure 3. Major Company Components and Relations\nAll these departments have a presence at both the global level and the local level, like at\nMicrosoft Turkey. Microsoft Turkey's role is to execute the global strategy in the local\nmarket, handling local sales, marketing, and support.\nOne of the most evident examples of this service system at work is the procedure that a\ncustomer completes to deploy a Virtual Machine (VM) on Microsoft Azure. The customer\nstarts by logging into the Azure portal, picking the service, and setting up the VM, by\nselecting specifications such as its size, operating system, and data center region. This\nsystem will then automatically validate these settings and then start the digital production\nprocess, the essence of which is provisioning. It takes only a few minutes to automatically\nallocate hardware, install the software and deploy the running VM, which is then available\nto the customer by Azure.\nThis involves non-productive and productive aspects. Those steps that directly create value\nare productive steps, e.g. the first configuration and the last running service. Conversely,\nnon-productive elements are required yet they are time the customer has to wait, such as in\nthe validation and deployment process. The performance objective of Microsoft is to reduce\nthis non-productive time by the continuous automation and efficiency increments.\nPerformance of the system is quantified by such metrics as deployment velocity and service\navailability, which is formally assured by Service Level Agreements (SLAs) that guarantee\nuptimes frequently over 99.9%. This whole service provision system is underpinned with\nadvanced planning and forecasting. Microsoft does not predict the sale of individual services\nbut the aggregate demand in various regions in computing power. This information, which is\ngrounded on the past use and market trends, is very essential in planning on where and\nwhen to construct new data centers to address future customer demands.\nMoreover, the company has a special kind of inventory — the unused capacity of servers\nand storage of its data centers. This is a key balancing act in managing this digital inventory\nbetween not having enough capacity to meet unexpected customer demand and the cost of\n7\n\nhaving idle hardware. All this is possible through a huge physical logistics effort that controls\nthe worldwide supply chain of server hardware, including the procurement and shipment of\nhardware, its installation and its eventual decommissioning. This is a complicated backend\nstructure that is the real physical infrastructure that enables Microsoft to provide the global\ncloud services it does.\nd. Profess",
        "start_idx": 7280,
        "end_idx": 14420,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "professional_ethical_1",
        "section_name": "d. Professional and Ethical Responsibilities of Engineers",
        "content": "ional and Ethical Responsibilities of Engineers\nProfessional and ethical responsibility of engineers is very significant in a global technology\ncompany such as Microsoft that produces products that touch the lives of billions of people.\nThe digital world is the creation of engineers, and their activity presupposes not only\ntechnical competence but also the devotion to the safety, privacy, and integrity. Engineers\nare expected to maintain such high standards in all their professional undertakings because\nthe reputation of the company and the trust of its customers is on the line.\nFunctions of Software Engineers at Microsoft\nDuring my internship, I have seen that Software Engineers at Microsoft carry out very\ndiverse roles that are critical in the product development process. Their major task is to\ndevelop and support the software and services the firm provides. They all have the following\nfunctions in common:\n• Designing and Developing Software: Creation of clean, efficient and maintainable\ncode of new products and services like Microsoft Azure or Office 365.\n• Testing and Debugging: The systematic testing of their code to find and eliminate\nbugs and this is what makes the software reliable and works as per the intended\npurpose.\n• Code Reviewing: Reading and reviewing the code of other members of the team to\nassure quality, any potential bugs, and security.\n• Collaboration: Working with other engineers, program managers and designers in\nthe planning, development and introduction of products.\n• Maintenance and Updates: This is the process of improving already installed\nsoftware to add new functionality, improve performance and customer feedback.\n• Documentation: Technical writing of how the software works, which will be used in\nfuture to maintain the software and other programmers.\nProfessional and Ethical Responsibilities\nOther than the technical roles, there are other professional and ethical roles of Software\nEngineers at Microsoft. Professionally, they are supposed to be competent by keeping up\nwith the changing technology, work diligently and be accountable to their code. Their\nobligations are deeper, even ethically. One of the main ethical responsibilities is to keep\nusers’ privacy which means that the enormous data processed by Microsoft systems should\n8\n\nbe safe. This is directly associated with the accountability of developing secure products that\nprotect against cyberattacks. Moreover, engineers have to be honest and act with integrity,\nand they should be transparent when it comes to software limitations.\nCompany Standards and Code of Conduct\nMicrosoft also has a formal document known as the Microsoft Standards of Business\nConduct to ensure that the employees are guided in the roles. It is an evident ethical\ndocument that would be applicable to the employees all over the world, including those in\nTurkey. It is based on the values of trust and integrity and deals with such significant\nprinciples as the compliance with the law, the secrecy of information, the establishment of\nthe respectful and non-discriminatory atmosphere at the workplace, and the lack of conflict\nof interest. The employees will be subjected to yearly training on the standards to ensure\nthat the employees understand these standards and comply with them.\nObserved Ethical Practices in Action\nI have seen these standards being applied in day to day engineering during my internship. As\nan example, the code review process is an effective way of conducting a practical check both\non the technical quality and possible security or privacy issues. I also noted that there was\nmuch emphasis on privacy during project planning meetings as teams discussed how to\ncollect less data and design it in such a way that user information was safeguarded. Lastly,\nefforts to reduce the possibility of bias in AI systems were also made, as teams will spend\ntime testing on different data to make sure that their technology will be fair to all. These\nexamples indicate that in Microsoft, ethical responsibilities are so ingrained into the\nengineering culture and is an essential component of the product development process.\n9\n\n",
        "start_idx": 14420,
        "end_idx": 18564,
        "level": 2,
        "parent_id": "company_sector_1"
      },
      {
        "section_id": "activity_analysis_1",
        "section_name": "2. Summer Practice Description",
        "content": "s. The essence of my\nwork was to develop a smart forecasting system based on large-scale historical financial\ndatasets consisting of millions of daily stock market records. My major engineering activities\nwere the development of various machine learning and deep learning models (including\nLSTM and GRU networks implemented in PyTorch, as well as baseline models such as\nARIMA) and the creation of an advanced preprocessing pipeline to clean data and optimize\nfeatures.\nThe most difficult part was to combine traditional statistical forecasting methods with\nmodern deep learning predictions to make a hybrid prediction system. Traditional\napproaches such as ARIMA and moving average models had to be integrated with neural\nnetwork-based methods, and this required careful architectural design and extensive testing\nto make the system compatible and effective.\nThis internship project was directly related to several fields of my computer science studies,\nespecially in the fields of artificial intelligence, software engineering, and financial data\nscience. The deep learning modules involved the application of concepts on statistical\nanalysis and algorithm optimization that I learned in my coursework, and the software\narchitecture was based on what I learned in my software engineering courses. The financial\nforecasting dimension exposed me to the practical uses of data science concepts that were\ninitially theoretical and gave me a practical understanding of time series modeling and data\nprocessing techniques that I had learned in my coursework.\nThere were many technical problems encountered in the project that enhanced my\nengineering skills tremendously. The need to use large-scale time series datasets\nnecessitated the acquisition of more sophisticated data processing methods and memory\noptimization principles that are not typically covered in academic institutions. Financial time\nseries analysis required knowledge of market behavior and feature engineering techniques\nbecause of its inherent complexity.\nTo get the best results in terms of prediction accuracy and processing speed, the deep\nlearning models had to be optimized thoroughly through trial and error with various\narchitectures and hyperparameter tuning in PyTorch. The combination of traditional\nstatistical forecasting and neural network-based approaches helped me to gain important\nlessons regarding system architecture and the interaction of its components. The complexity\nof mixing classical models with probabilistic deep learning predictions had to be addressed\nwith extreme care when considering edge cases, error handling, and the design of the user\ninterface.\nI also gained experience with professional development tools such as version control\nsystems, automated testing frameworks, and deployment pipelines that are common in the\nfinancial technology sector.\nThis experience provided me with a detailed insight into the industry-level practices of\nsoftware development and the concepts of AI-driven financial forecasting. The project\n10\n\nenhanced my problem-solving and technical communication skills tremendously since I gave\npresentations to senior engineers and stakeholders frequently.\nAbove all, the internship gave me an understanding of the way in which academic expertise\ncan be applied to practical engineering solutions and taught me that one should never stop\nlearning in the fast-changing world of artificial intelligence and financial technology.\n• Activity Analysis\nOne of the core computer en",
        "start_idx": 18830,
        "end_idx": 22329,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "activity_analysis_2",
        "section_name": "• Activity Analysis",
        "content": "gineering activities that I had a hand in was designing and\ndeveloping a multi-stage stock price prediction pipeline. This was the most significant part of\nthe system intelligence and the one that is ultimately making the final prediction for any\ngiven stock. This task was an independent engineering activity consisting of system design,\nalgorithm incorporation, and data flow.\nThe users first select a stock symbol and a time period, and then the system loads the\ncorresponding historical price data automatically from a financial data source. Raw data is\nthen subjected to data cleaning and normalization. Missing values are handled, outliers are\nsmoothed, and time series are normalized to display consistent performance under different\nmarket conditions. This is the first step towards performance and reliability, as it ensures\nthat the following steps do not have to deal with noisy or varying input.\nUpon successful completion of initial quality checks, the dataset moves into a parallel\nmodeling phase, where it is handled by both conventional time series models and deep\nlearning models in parallel. This parallel approach increases resilience and serves as two\nindependent sources of predictive evidence.\n• The statistical engine utilizes standard forecasting models such as ARIMA and\nmoving average models to detect linear trends and seasonality.\n• The deep learning engine, implemented with LSTM and GRU networks in PyTorch,\nuncovers complex non-linear patterns in the time series.\nLastly, the system combines the outputs of these two engines to generate a final prediction\nthrough a confidence scoring and ensemble mechanism, providing an informed estimate of\nthe next time steps of the stock price.\nPerformance Indicators and Metrics\nTo measure the performance and efficiency of this forecasting pipeline, I set up and tracked\nkey performance indicators (KPIs). These were important to identify the strengths and\nweaknesses of the system and to guide iterative development.\n• Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE): to compute the\naverage magnitude of the prediction error.\n• Mean Absolute Percentage Error (MAPE): to quantify prediction accuracy in\npercentage terms, making it possible to compare different stocks easily.\n• R² Score: to evaluate the goodness-of-fit of the models in explaining the variance of\nstock price movement.\n• Computation Time per Forecast: as an explicit measure of system performance and\nuser experience.\n11\n\nBecause financial time series are generally highly non-stationary and volatile, I also\nmonitored rolling-window valida",
        "start_idx": 22329,
        "end_idx": 24920,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "project_proposed_1",
        "section_name": "• Project",
        "content": "tion statistics to ensure that the models were working well\nunder different market regimes.\nProblematic Issues and Analysis\nThe system design and performance measures were extensively analyzed, and a number of\nproblematic issues were identified which must be improved in the future:\n• Concept Drift in Financial Markets:\nMarket behavior changes due to macroeconomic factors, regulatory changes, or\nother unexpected occurrences. Thus, models trained on historical data may lose\ntheir predictive power over time. An official model maintenance process is\nnecessary to retrain, validate, and release updated models periodically to adapt to\nchanging market conditions.\n• Sensitivity to Sudden Market Shocks:\nIncidents such as financial crises or unforeseen company-specific news can induce\nsudden price movements that are difficult to anticipate for both ARIMA and LSTM\nmodels. Integration of real-time news sentiment or other alternative data inputs can\nhelp address this limitation.\n• Data Quality and Latency Issues:\nReal-time market data feeds may occasionally contain missing or delayed data\npoints. Without strong data validation and error management, these issues can\ndegrade prediction quality.\n• Scalability of Forecasting Under High Load:\nWhen numerous users request forecasts at the same time, the existing synchronous\nforecasting pipeline can become a bottleneck. To enhance scalability, an\nasynchronous task queue (e.g., utilizing Celery and RabbitMQ) could be\nimplemented to handle forecasting requests without affecting the responsiveness of\nthe main application.\n• Project\nI worked on a Stock Price Predictor sing PyTorch.\nThis project uses LSTM and GRU machine learning models to predict the next day price of a\ncompany share.\nThe user can select a stock code (for example AAPL for Apple), a start date and an end date.\nThe system then shows the predicted price and also the real price.\n12\n\nFigure 4\nTo understand the performance of the models we used simple metrics like mean squared\nerror and validation loss.\nFrom the graphs the loss goes down in the first 10 epochs and then becomes stable.\nThis means the models learned the pattern.\nFigure 5\nWhen we tested with Apple (AAPL) data from 2020 to 2025 we found that sometimes the\nGRU model prediction had bigger difference from real price.\n",
        "start_idx": 24920,
        "end_idx": 27223,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "learning_5",
        "section_name": "For some dates, the prediction was lower or higher than market more than 5%.\n13\n\nThis difference can be seen on 30 September 2025 where LSTM predicted $241.45 but real\nprice was $255.46.\nFigure 6\nThe GRU model’s accuracy is not always stable.\nThis is a computer engineering problem: the model needs better accuracy for financial\ndecisions.\nBy checking literature, some possible solutions are: use more features like trading volume or\nmarket news sentiment, apply regularization techniques like dropout to reduce overfitting,\nor combine LSTM and GRU in a hybrid model.\nAdding more features and using dropout gave a small improvement of validation loss.\n14\n\nFigure 7\nThese pictures show how the user chooses dates and runs the prediction.\nThis work shows how to use time series data, evaluate machine learning models, and look\nfor improvement methods from academic sources.\n15",
        "content": "For some dates, the prediction was lower or higher than market more than 5%.\n13\n\nThis difference can be seen on 30 September 2025 where LSTM predicted $241.45 but real\nprice was $255.46.\nFigure 6\nThe GRU model’s accuracy is not always stable.\nThis is a computer engineering problem: the model needs better accuracy for financial\ndecisions.\nBy checking literature, some possible solutions are: use more features like trading volume or\nmarket news sentiment, apply regularization techniques like dropout to reduce overfitting,\nor combine LSTM and GRU in a hybrid model.\nAdding more features and using dropout gave a small improvement of validation loss.\n14\n\nFigure 7\nThese pictures show how the user chooses dates and runs the prediction.\nThis work shows how to use time series data, evaluate machine learning models, ",
        "start_idx": 27223,
        "end_idx": 28039,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "conclusion_1",
        "section_name": "Figure 4",
        "content": "and look\nfor improvement methods from academic sources.\n15\n\na. Impact",
        "start_idx": 28039,
        "end_idx": 28108,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "conclusion_6",
        "section_name": "a. Impact",
        "content": "\nThe stock price prediction system that was developed in this project can influence the world\nin many different ways.\nIts effects can be seen in global, economic, environmental, and social areas.\nThese impacts show why computer engineering work is important for people outside of the\ntechnical field.\nGlobal impact\nStock markets work across the whole world.\nWhen a system like this is placed on the internet, people in many countries can use it to get\npredictions for their own local markets or for international stocks.\nFor example, an investor in Turkey, Germany or Japan can open the web page and see a\nforecast for a company like Apple or Amazon in seconds.\nThis gives all users access to the same level of information no matter where they live.\nIt also supports more fair and transparent decision making in the global economy because\ngood data is not limited only to large banks or rich investors.\nEconomic impact\nAccurate predictions help people and companies reduce their financial risk.\nIf investors can plan their buying and selling with better knowledge, they can protect their\nmoney during sudden market changes.\nBig companies can also use these forecasts to plan budgets and investments.\nA tool like this creates new business opportunities for financial technology companies and\nfor data scientists who can design and maintain such systems.\nNew services and jobs can appear around the system, for example consulting services that\nhelp small investors use the forecasts in their own investment strategies.\nEnvironmental impact\nThe system runs fully in a digital environment.\nIt does not need paper documents or physical transport of information.\nThis reduces paper waste and the carbon footprint that comes from printing and mailing\nreports.\nBig cloud providers such as Microsoft also invest in renewable energy for their data centres,\nso running the system on cloud infrastructure can be more eco-friendly than traditional\nfinancial reporting methods.\nWhen many companies use such digital tools, the total reduction in paper and energy use\nbecomes important for the planet.\nSocial impact\nSmall investors usually do not have the same resources as big financial institutions.\nThis system gives them a tool that works almost like professional software.\nIt makes it easier for more people to join the financial markets and increases financial\ninclusion.\nBut there is also a responsibility: users’ personal data and market behaviour must stay\nprivate, and the predictions must be explained clearly so that nobody is misled.\nMaintaining transparency and privacy builds trust and shows that engineering solutions can\nsupport society in a fair way.\n16\n\nb. Team Work\nThe project was mainly developed alone, with help and guidance from a mentor, a senior\nsoftware engineer.\nTeam roles and contribution\n• Mentor (CSA Manager) – gave advice about deep learning models and checked the\ncode in regular meetings.\n• Intern (Computer Engineering, me) – collected financial data, built ARIMA, LSTM\nand GRU models, created the Flask web interface, and joined all parts of the project\nfrom start to finish.\nEvaluation\nThe team was small but effective. The mentor gave weekly feedback and answered\nquestions. There was freed",
        "start_idx": 28108,
        "end_idx": 31323,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "conclusion_2",
        "section_name": "b. Team Work",
        "content": "om to work independently but also professional advice when\nneeded. This balance helped to complete the project with good quality.\nc. Life Long Learning\nThis project shows clearly that learning never stops for a computer engineer.\nTechnology changes very fast and the skills from university are only a starting point.\nWhile building the stock price prediction system many new topics had to be learned and will\ncontinue to be important in the future.\nKnowledge and skills to improve\nWorking with large financial time series required a deeper understanding of time series\nmodelling.\nI had to learn methods such as ARIMA and how to analyse seasonality and trends.\nAt the same time, deep learning models like LSTM and GRU needed more practice.\nTuning hyper-parameters, choosing the right network depth and handling overfitting were\nall areas where extra study was necessary.\nBasic knowledge of ",
        "start_idx": 31323,
        "end_idx": 32212,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "conclusion_3",
        "section_name": "c. Life Long Learning",
        "content": "how financial markets work and how prices react to economic events\nwas also important to understand the data correctly.\nThese are skills that were not fully covered in normal university courses, so self-study was\nessential.\nHow the information was collected\nTo gain these skills, I searched and read many different sources.\nI looked at academic papers and online tutorials that explained how LSTM and GRU can be\nused for time series forecasting.\nI checked open-source code on Kaggle and GitHub to see how other developers solved\nsimilar problems.\nThe official PyTorch documentation helped me understand the correct way to implement\nthe models.\nWhen I needed information about the behaviour of stock markets, I used articles from\nfinancial websites and reports from trusted financial research companies.\nAll this material was freely available on the internet, but it required careful reading and\npractice to apply it correctly.\n17\n\nAttitude to lifelong learning\nThis experience taught that a computer engineer must always be ready to learn.\nNew algorithms, new tools and new problems appear every year.\nTo stay current, it is necessary to follow online courses, read new research papers and join\ndeveloper communities such as Stack Overflow or the PyTorch forum.\nThis continuous learning is not only good for professional growth but also for personal\ndevelopment, because it helps to solve complex problems and adapt to changes in\ntechnology.\nWith this attitude, it is possible to keep building better systems and to stay valuable in the\nfast-moving world of artificial intelligence and financial technology.\n18\n\n3. Conclusions\nThis project demonstrated how advanced computer engineering methods can be employed\nto solve a challenging real-world problem: forecasting stock prices in a highly volatile and\nnon-stationary market. Creating a multi-stage prediction pipeline that combined\nconventional time series analysis and state-of-the-art deep learning models took skill from\nmany areas—data preprocessing, algorithm development, software architecture, and model\nassessment.\nThe study proved that using the combination of ARIMA and moving average models and\nLSTM and GRU networks was more accurate in making predictions compared to a single\ntechnique. Proper preprocessing and normalization were crucial in ensuring the quality of\ndata, and critical performance indicators such as MAE, RMSE and MAPE allowed for constant\nc",
        "start_idx": 32212,
        "end_idx": 34634,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "conclusions_1",
        "section_name": "3. Conclusions",
        "content": "hecks on accuracy and stability. The exercise also highlighted key limitations of current\nmethods, including concept drift, sensitivity to sudden market shocks, and the need for\nscalable, low-latency prediction when numerous users are active at the same time.\nThese issues highlight the importance of ongoing maintenance and innovation. Frequent\nretraining, integration with real-time sentiment streams, and asynchronous processing\narchitectures are favorable avenues. In addition to the technical insights, the project also\nprovided valuable knowledge of the realities of big financial data engineering and showed\nhow concepts in academia can be transformed into industrial application. The project\ncontributed to technical as well as analytical skills and gave a clear idea of how computer\nengineering contributes to decision-making in modern financial technology.\n19\n\nReferences\nMicrosoft, 2024. Microsoft Annual Report 2024. [online] Microsoft Corporation.\nAvailable at: https://www.microsoft.com/investor/reports/ar24/ [Accessed 1 August 2025].\nGartner, 2023. Market Share Analysis: Cloud Infrastructure and Platform Services,\nWorldwide, 2023. [online] Gartner.\nAvailable at: https://www.gartner.com/en [Accessed 1 August 2025].\nIDC, 2023. Worldwide Public Cloud Services Spending Guide. [online] International Data\nCorporation.\nAvailable at: https://www.idc.com [Accessed ",
        "start_idx": 34634,
        "end_idx": 36012,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "references_1",
        "section_name": "References",
        "content": "8 August 2025].\n20\n\nDay 1:\nToday was my very first day of the Microsoft online internship. I joined the session\nfrom my room using Teams. At the beginning, I was nervous because everything was\nonline and I had never done a remote internship before. The mentors introduced\nthemselves and explained the plan: AI training, PyTorch exercises, and a stock price\nprediction project. I liked that even though we couldn’t meet in person, the mentors\nencouraged questions and said we could always write in chat. I spent some time\nadjusting my workspace and making sure my computer was ready for coding. Even\nthough it ",
        "start_idx": 36012,
        "end_idx": 36621,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "daily_activities_1",
        "section_name": "Day 1:",
        "content": "feel strange to meet people only through video calls, I feel excited and\nready to start learning.\nDate Supervisor’s Name Signature\n04.08.2025 Barbaros Günay\nDay 2:\nWe had our first lecture on Artificial Intelligence today. The trainer explained what is\nAI, how machine learning works, and why deep learning is important. At first, it was\na little confusing, especially remote, because I couldn’t always follow every example\nimmediately. But I paused the recording and tried to understand each concept. I also\ntook notes in a digital notebook, which helped me organize my thoughts. I was\nsurprised to learn how AI is used everywhere from social media to stock market\npredictions. After the session, we had a small quiz to check understanding. I felt\nmotivated because I could see how this knowledge could be applied in real projects.\n21\n\nDate Supervisor’s Name Signature\n05.08.2025 Barbaros Günay\nDay 3:\nToday we started learning PyTorch. Mr. Barbaros shared his screen, and we followed\nalong step by step. First of all, the syntax appeared like a little strange, but it was fun\nto play with tensors. I spend some time after class experimenting with operations,\nand I even tried creating a tensor in a slightly different way than shown, just to see\nwhat happens. I made a few errors but I learned by fixing them. I liked that I could\ntest everything on my own computer and go at my own pace. Even though we\nweren’t in a classroom, I feel like I was really part of the team because we shared\ntips in the online chat.\nDate Supervisor’s Name Signature\n06.08.2025 Barbaros Günay\n22\n\nDay 4:\nThis day was more technical. We learned how to build simple neural networks. The\nmentor explained layers, activation functions, and the idea of backpropagation. I\ntook screenshots of the diagrams because I wanted to study them later. At first, I\nwas confused by the math, but when I coded the network myself and ran it, I\nunderstood much better. Seeing the model learn, even on a tiny dataset, was very\nsatisfying. I also shared my questions in the group chat, and some classmates had\nthe same problems. It was interesting to see how even online, people could help\neach other. At the end of the day, I feel more confident in using PyTorch.\nDate Supervisor’s Name Signature\n07.08.2025 Barbaros Günay\nDay 5:\nToday we were introduced to our main project: predicting stock prices using LSTM\nand GRU models. The mentor explained that these models are very good for time\nseries data because they can remember patterns from previous days. We may do\ndifferent projects but the reason I decided to do this project is my curiosity about\nmachine learning and AI. I felt excited but also a little nervous, because predicting\nstock prices seemed very challenging. We had a group meeting online to discuss\ntasks, and I suggested that we start by exploring the data carefully before building\nthe models. Even though we communicated only through video calls and chat, I\ncould feel the teamwork. I spent the evening thinking about possible approaches and\nmaking notes for the first steps. I couldn’t wait to start coding tomorrow.\n23\n\nDate Supervisor’s Name Signature\n08.08.2025 Barbaros Günay\nDay 6:\nToday we concentrated on gathering stock data for our project. Because the entire\nprocess was online, I ended up opening several browser tabs and working with\nvarious APIs on my computer. I relied on Yahoo Finance along with a few Python\nscripts to pull historical stock prices. At first, a few files contained missing values or\nodd formatting, which was a bit frustrating. I spent quite some time cleaning the\ndata, fixing errors, and checking for inconsistencies. I also shared a few tips with my\nteammates in our group chat, such as using pandas functions to manage missing\nvalues. Even though we weren’t physically in the same room, it still felt like we were\ntackling the challenges together. By the end of the day, I had a clean dataset ready\nfor preprocessing and felt genuinely proud of the work.\nDate Supervisor’s Name Signature\n24\n\n11.08.2025 Barbaros Günay\nDay 7:\nToday was spent preparing the data for the models. We also learned how to\nnormalize stock prices and divide the data set into training and test data sets. I\nexperimented with different ways of scaling and realized that subtle change in\npreprocessing could change the model’s result. I made a few mistakes with indexing\nthat led to a few errors but fixing them taught me a lot. I also plotted the stock\ntrends with matplotlib because looking at the graphs made me notice the patterns\n",
        "start_idx": 36621,
        "end_idx": 41148,
        "level": 1,
        "parent_id": null
      },
      {
        "section_id": "daily_activities_7",
        "section_name": "Day 8:",
        "content": "more evidently. Even over the internet, I liked that we could communicate different\napproaches by the group chat and share one another’s insights.\nDate Supervisor’s Name Signature\n12.08.2025 Barbaros Günay\nDay 8:\nToday we started out learning about LSTM networks. The instructor mentioned that\nLSTM retains information from days past, perfect for stock forecasting. I was a bit\nconfused by the concept of “cell state” and “gates,” but I made minuscule notes and\ndiagrams for myself. I coded a simple LSTM model on my Jupyter Notebook and\nwatched the model work on sequences of stock data. Seeing the model learn\nsequentially was very exhilarating. I also experimented by changing the layers and\nthe units and viewing the outcome. Online classes do tax you, but I felt that doing\nexperiments by myself made me comprehend the concepts way better.\n25\n\nDate Supervisor’s Name Signature\n13.08.2025 Barbaros Günay\nDay 9:\nAfter LSTM, we learned GRU models today. The professor stated that GRU is not\nmore complicated than LSTM but will perform equally well for the majority of\nproblems. I made a GRU model and compared it with yesterday's model of LSTM. I\nnoted that GRU trains faster but sometimes yields only marginally better predictions\nthan LSTM. I also shared my results on our online discussion board and got\ncomments from fellow peers. I felt proud that I could recognize differences by\nmyself. Playing around with hyperparameters was enjoyable and interesting to see\nthe effects on predictions.\nDate Supervisor’s Name Signature\n26\n\n14.08.2025 Barbaros Günay\nDay 10:\nToday we trained our first real models with stock data. That took a while because my\nmachine was pretty slow, and training LSTM and GRU models was a challenge of\npatience. I religiously checked the loss values decreasing, and it felt good that the\nmodel was improving. The predictions weren't that good yet, but I felt good looking\nat the results. I shared pictures with my team members, and we discussed areas of\nimprovement for next time. I did more time testing varying batch size and sequence\nlength. Though the day was tiring, I felt satisfied and motivated to improve my\nmodels even more.\nDate Supervisor’s Name Signature\n15.08.2025 Barbaros Günay\nDay 11:\nToday I spent on hyperparameter tuning. I could see how essential is the fine-tuning\nof parameters like learning rate, batch size, and epochs. At first I only used default\nparameters but was encouraged by my supervisor to experiment. I set up a small\ntable that enabled me comparing results for different settings. Now sometimes the\ncurve of the loss was better, sometimes worse. I could recognize that the best\ncombination is quite like an experimental approach but also requires sensible\nreasoning. I felt rather exhausted running the models again and again but felt like a\nreal scientist testing different hypotheses.\n27\n\nDate Supervisor’s Name Signature\n18.08.2025 Barbaros Günay\nDay 12:\nToday I spent the day visualizing the model's predictions. I plotted the actual stock\nprices versus the forecasted stock prices on the same graph with matplotlib. I\nenjoyed being able to view how close the two lines were at some places but also\nhow far off they could be at another. I experimented with coloring things differently\nand including labels just to make the graphs more interpretable. Getting visuals out\nthere in our chat made it easier for everyone to communicate because there was no\nconfusion over the outcome. I also realized that good visualization is almost as\nimportant as building the model itself because that enables you to explain your\nresults.\nDate Supervisor’s Name Signature\n28\n\n19.08.2025 Barbaros Günay\nDay 13:\nAfter checking the results, I tried today to fine-tune the quality of the model. I\ndeepened the layers of the LSTM and tried optimizers like Adam and SGD. During\nother occasions, the modifications made the model worse, frustrating me, but I\nallowed that because that’s learning. I also scoured the internet for publications\nrelated to time series prediction, from which I derived new insights related to\nfeature engineering. I made a note of them for future use. Though the model was\nstill not perfect, I felt good about myself because I was learning to think creatively\nand analytically.\nDate Supervisor’s Name Signature\n20.08.2025 Barbaros Günay\nDay 14:\nToday we had a longer group online discussion. Everyone reported their progress,\nand I discussed my experiments with changing optimizers and extra layers. Everyone\nreported their experiences too, and I could not believe the way we tried different\napproaches. We also discussed splitting work at the end f",
        "start_idx": 41148,
        "end_idx": 45769,
        "level": 2,
        "parent_id": "daily_activities_1"
      },
      {
        "section_id": "internship_documents_1",
        "section_name": "Internship Documents",
        "content": "or the final presentation. I\noffered to do more of the visualization and description slides, because I enjoy\nexplaining results nicely. Though we were just discussing on Teams, I felt very strong\nteam spirit. It was good to realize that we weren't only coding but also learning to\nwork as a team.\n29\n\nDate Supervisor’s Name Signature\n21.08.2025 Barbaros Günay\nDay 15:\nToday I started preparing the end version of the model for our project. I used some\nof the best of earlier experiments: using GRU with a specific learning rate and\nplotting the forecast with good graphs. I also started writing brief description of the\nprocedure, such that we could use it later while giving our presentation. The mentor\nalso reminded that we are not after forecasting stock prices precise",
        "start_idx": 45769,
        "end_idx": 46542,
        "level": 1,
        "parent_id": null
      }
    ]
  },
  "source_metadata": {
    "total_length": 50970,
    "extraction_timestamp": "2025-11-11T14:57:17.226276",
    "chunked": true,
    "chunk_count": 4,
    "fixed": true,
    "fix_timestamp": "2025-11-11T14:57:17.229236"
  }
}